{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d81f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ef875c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "from scipy.special import comb\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "699c40c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('1', 'REGULAR'): {'mu': 4.832017135463374,\n",
       "  'sigma': 0.006958469079704907,\n",
       "  'loc': -18.81216413263904},\n",
       " ('1', 'SPECIFIC'): {'mu': 4.6755415208149325,\n",
       "  'sigma': 0.011323646523065594,\n",
       "  'loc': -6.43006717763892},\n",
       " ('2', 'REGULAR'): {'mu': 5.0125526231624855,\n",
       "  'sigma': 0.005548307071802354,\n",
       "  'loc': -8.97654648869145},\n",
       " ('2', 'SPECIFIC'): {'mu': 4.617186601308905,\n",
       "  'sigma': 0.010134016947128502,\n",
       "  'loc': -4.806412598886724},\n",
       " 'REGULAR': {'mu': 3.7589339214345716,\n",
       "  'sigma': 0.024480564633599006,\n",
       "  'loc': -2.846758403840244},\n",
       " 'SPECIFIC': {'mu': 4.650110366782741,\n",
       "  'sigma': 0.01083819523964822,\n",
       "  'loc': -5.260938954957395},\n",
       " 'REGULAR_x4': {'mu': 5.145228282554465,\n",
       "  'sigma': 0.006121000886257698,\n",
       "  'loc': -11.387033615361277},\n",
       " 'SPECIFIC_x4': {'mu': 6.036404727902634,\n",
       "  'sigma': 0.002709623408600959,\n",
       "  'loc': -21.043755819830228}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pickles/anonbank_lognorm_params.pkl', 'rb') as f:\n",
    "    lognorm_params_df = pickle.load(f)\n",
    "    \n",
    "lognorm_params_dict = lognorm_params_df.set_index('param_name').T.to_dict()\n",
    "\n",
    "lognorm_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19db4bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('1', 'REGULAR'): {'mean_service_time': 12.641068214208262,\n",
       "  'std': 274.09685054233046},\n",
       " ('1', 'SPECIFIC'): {'mean_service_time': 14.229164478664519,\n",
       "  'std': 447.916748807343},\n",
       " ('2', 'REGULAR'): {'mean_service_time': 19.02417781821472,\n",
       "  'std': 344.9407640049321},\n",
       " ('2', 'SPECIFIC'): {'mean_service_time': 15.020757462895316,\n",
       "  'std': 257.32365209990894}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pickles/anonbank_service_rate.pkl', 'rb') as f:\n",
    "    service_rate = pickle.load(f)\n",
    "\n",
    "# Convert service_rate to a dictionary for easier access\n",
    "ser_rate_dict = service_rate.set_index(['group', 'type_group'])[['mean_service_time', 'std']].to_dict('index')\n",
    "\n",
    "ser_rate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ba74925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global variables\n",
    "\n",
    "# Queue size\n",
    "MAX_QUEUE_SIZE = 20\n",
    "\n",
    "# Discount factor for future rewards\n",
    "DISCOUNT = 0.95\n",
    "\n",
    "# Penalty for dropping calls (when queue is full)\n",
    "DROP_PENALTY = 3600.0 # 1 hour in seconds\n",
    "\n",
    "# State space: (queue 1 size, queue 2 size, call type)\n",
    "STATE_SPACE = (MAX_QUEUE_SIZE + 1, MAX_QUEUE_SIZE + 1, 2)\n",
    "\n",
    "# Call type: 0 = regular, 1 = specific\n",
    "CALL_TYPE = [0, 1]\n",
    "\n",
    "# Actions: 0 = route to queue 1, 1 = route to queue 2\n",
    "ACTIONS = [1, 2]\n",
    "\n",
    "# Arrival parameters for regular and specific calls\n",
    "ARRIVAL_REGULAR_MU = lognorm_params_dict['REGULAR']['mu']\n",
    "ARRIVAL_REGULAR_SIGMA = lognorm_params_dict['REGULAR']['sigma']\n",
    "ARRIVAL_REGULAR_LOC = lognorm_params_dict['REGULAR']['loc']\n",
    "ARRIVAL_SPECIFIC_MU = lognorm_params_dict['SPECIFIC']['mu']\n",
    "ARRIVAL_SPECIFIC_SIGMA = lognorm_params_dict['SPECIFIC']['sigma']\n",
    "ARRIVAL_SPECIFIC_LOC = lognorm_params_dict['SPECIFIC']['loc']\n",
    "\n",
    "SERVICE_REGULAR_1_MU = lognorm_params_dict[('1', 'REGULAR')]['mu']\n",
    "SERVICE_REGULAR_1_SIGMA = lognorm_params_dict[('1', 'REGULAR')]['sigma']\n",
    "SERVICE_REGULAR_1_LOC = lognorm_params_dict[('1', 'REGULAR')]['loc']\n",
    "SERVICE_SPECIFIC_1_MU = lognorm_params_dict[('1', 'SPECIFIC')]['mu']\n",
    "SERVICE_SPECIFIC_1_SIGMA = lognorm_params_dict[('1', 'SPECIFIC')]['sigma']\n",
    "SERVICE_SPECIFIC_1_LOC = lognorm_params_dict[('1', 'SPECIFIC')]['loc']\n",
    "\n",
    "SERVICE_REGULAR_2_MU = lognorm_params_dict[('2', 'REGULAR')]['mu']\n",
    "SERVICE_REGULAR_2_SIGMA = lognorm_params_dict[('2', 'REGULAR')]['sigma']\n",
    "SERVICE_REGULAR_2_LOC = lognorm_params_dict[('2', 'REGULAR')]['loc']\n",
    "SERVICE_SPECIFIC_2_MU = lognorm_params_dict[('2', 'SPECIFIC')]['mu']\n",
    "SERVICE_SPECIFIC_2_SIGMA = lognorm_params_dict[('2', 'SPECIFIC')]['sigma']\n",
    "SERVICE_SPECIFIC_2_LOC = lognorm_params_dict[('2', 'SPECIFIC')]['loc']\n",
    "\n",
    "def mean_shifted_lognormal(mu, sigma, loc):\n",
    "    \"\"\"\n",
    "    Calculate mean of shifted lognormal distribution:\n",
    "    E[X] = exp(mu + sigma^2 / 2) + loc\n",
    "    \"\"\"\n",
    "    return np.exp(mu + (sigma**2) / 2) + loc\n",
    "\n",
    "def std_shifted_lognormal(mu, sigma, loc):\n",
    "    \"\"\"\n",
    "    Calculate standard deviation of shifted lognormal distribution:\n",
    "    The location (loc) shifts the mean but does not affect the std.\n",
    "    \"\"\"\n",
    "    return np.sqrt((np.exp(sigma**2) - 1) * np.exp(2*mu + sigma**2))\n",
    "\n",
    "# Arrival rates\n",
    "\n",
    "ARRIVAL_REGULAR = mean_shifted_lognormal(\n",
    "    ARRIVAL_REGULAR_MU,\n",
    "    ARRIVAL_REGULAR_SIGMA,\n",
    "    ARRIVAL_REGULAR_LOC\n",
    ")\n",
    "\n",
    "ARRIVAL_SPECIFIC = mean_shifted_lognormal(\n",
    "    ARRIVAL_SPECIFIC_MU,\n",
    "    ARRIVAL_SPECIFIC_SIGMA,\n",
    "    ARRIVAL_SPECIFIC_LOC\n",
    ")\n",
    "\n",
    "ARRIVAL_REGULAR_STD = std_shifted_lognormal(\n",
    "    ARRIVAL_REGULAR_MU,\n",
    "    ARRIVAL_REGULAR_SIGMA,\n",
    "    ARRIVAL_REGULAR_LOC   \n",
    ")\n",
    "\n",
    "ARRIVAL_SPECIFIC_STD = std_shifted_lognormal(\n",
    "    ARRIVAL_SPECIFIC_MU,\n",
    "    ARRIVAL_SPECIFIC_SIGMA,\n",
    "    ARRIVAL_SPECIFIC_LOC\n",
    ")\n",
    "\n",
    "# Service means and std\n",
    "\n",
    "SERVICE_REGULAR_1 = ser_rate_dict[('1', 'REGULAR')]['mean_service_time']\n",
    "SERVICE_SPECIFIC_1 = ser_rate_dict[('1', 'SPECIFIC')]['mean_service_time']\n",
    "SERVICE_REGULAR_2 = ser_rate_dict[('2', 'REGULAR')]['mean_service_time']\n",
    "SERVICE_SPECIFIC_2 = ser_rate_dict[('2', 'SPECIFIC')]['mean_service_time']\n",
    "\n",
    "SERVICE_REGULAR_1_STD = ser_rate_dict[('1', 'REGULAR')]['std']\n",
    "SERVICE_SPECIFIC_1_STD = ser_rate_dict[('1', 'SPECIFIC')]['std']\n",
    "SERVICE_REGULAR_2_STD = ser_rate_dict[('2', 'REGULAR')]['std']\n",
    "SERVICE_SPECIFIC_2_STD = ser_rate_dict[('2', 'SPECIFIC')]['std']\n",
    "\n",
    "# SERVICE_REGULAR_1 = mean_shifted_lognormal(\n",
    "#     SERVICE_REGULAR_1_MU,\n",
    "#     SERVICE_REGULAR_1_SIGMA,\n",
    "#     SERVICE_REGULAR_1_LOC\n",
    "# )\n",
    "\n",
    "# SERVICE_REGULAR_2 = mean_shifted_lognormal(\n",
    "#     SERVICE_REGULAR_2_MU,\n",
    "#     SERVICE_REGULAR_2_SIGMA,\n",
    "#     SERVICE_REGULAR_2_LOC\n",
    "# )\n",
    "\n",
    "# SERVICE_SPECIFIC_1 = mean_shifted_lognormal(\n",
    "#     SERVICE_SPECIFIC_1_MU,\n",
    "#     SERVICE_SPECIFIC_1_SIGMA,\n",
    "#     SERVICE_SPECIFIC_1_LOC\n",
    "# )\n",
    "\n",
    "# SERVICE_SPECIFIC_2 = mean_shifted_lognormal(\n",
    "#     SERVICE_SPECIFIC_2_MU,\n",
    "#     SERVICE_SPECIFIC_2_SIGMA,\n",
    "#     SERVICE_SPECIFIC_2_LOC\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79303f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEANS:\n",
      "ARRIVAL_REGULAR: 40.068763243462236, ARRIVAL_SPECIFIC: 99.34173344076834\n",
      "SERVICE_REGULAR_1: 12.641068214208262, SERVICE_SPECIFIC_1: 14.229164478664519\n",
      "SERVICE_REGULAR_2: 19.02417781821472, SERVICE_SPECIFIC_2: 15.020757462895316\n",
      "STD:\n",
      "ARRIVAL_REGULAR: 1.050753626188135, ARRIVAL_SPECIFIC: 1.1337374798875093\n",
      "SERVICE_REGULAR_1: 274.09685054233046, SERVICE_SPECIFIC_1: 447.916748807343\n",
      "SERVICE_REGULAR_2: 344.9407640049321, SERVICE_SPECIFIC_2: 257.32365209990894\n"
     ]
    }
   ],
   "source": [
    "print(\"MEANS:\")\n",
    "\n",
    "print(f\"ARRIVAL_REGULAR: {ARRIVAL_REGULAR}, ARRIVAL_SPECIFIC: {ARRIVAL_SPECIFIC}\")\n",
    "print(f\"SERVICE_REGULAR_1: {SERVICE_REGULAR_1}, SERVICE_SPECIFIC_1: {SERVICE_SPECIFIC_1}\")\n",
    "print(f\"SERVICE_REGULAR_2: {SERVICE_REGULAR_2}, SERVICE_SPECIFIC_2: {SERVICE_SPECIFIC_2}\") \n",
    "\n",
    "print(\"STD:\")\n",
    "\n",
    "print(f\"ARRIVAL_REGULAR: {ARRIVAL_REGULAR_STD}, ARRIVAL_SPECIFIC: {ARRIVAL_SPECIFIC_STD}\")\n",
    "print(f\"SERVICE_REGULAR_1: {SERVICE_REGULAR_1_STD}, SERVICE_SPECIFIC_1: {SERVICE_SPECIFIC_1_STD}\")\n",
    "print(f\"SERVICE_REGULAR_2: {SERVICE_REGULAR_2_STD}, SERVICE_SPECIFIC_2: {SERVICE_SPECIFIC_2_STD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efeeaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lognormal expected value calculation\n",
    "\n",
    "def expected_waiting_time_binom_kingman(q_size, p_regular, mean_r, mean_s, rho, ca, cs):\n",
    "    \"\"\"\n",
    "    Calculate expected waiting time for a queue with q_size calls,\n",
    "    each call being regular with probability p_regular,\n",
    "    using direct mean service times for regular and specific calls.\n",
    "    \"\"\"\n",
    "    # Expected waiting time\n",
    "    EWq = 0.0\n",
    "    for k in range(q_size + 1):\n",
    "        # Binomial probability of k regular calls in the queue\n",
    "        prob = comb(q_size, k) * (p_regular ** k) * ((1 - p_regular) ** (q_size - k))\n",
    "        # Waiting time if k regular and (q_size - k) specific ahead\n",
    "        wait_k = k * mean_r + (q_size - k) * mean_s\n",
    "        EWq += prob * wait_k\n",
    "        \n",
    "    # Kingman scaling\n",
    "    Km = rho / (1 - rho) * (ca ** 2 + cs ** 2) / 2\n",
    "\n",
    "    return EWq * Km\n",
    "\n",
    "def expected_waiting_time_binom(q_size, p_regular, mean_r, mean_s):\n",
    "    \"\"\"\n",
    "    Calculate expected waiting time for a queue with q_size calls,\n",
    "    each call being regular with probability p_regular,\n",
    "    using direct mean service times for regular and specific calls.\n",
    "    \"\"\"\n",
    "    # Expected waiting time\n",
    "    wait = 0.0\n",
    "    for k in range(q_size + 1):\n",
    "        # Binomial probability of k regular calls in the queue\n",
    "        prob = comb(q_size, k) * (p_regular ** k) * ((1 - p_regular) ** (q_size - k))\n",
    "        # Waiting time if k regular and (q_size - k) specific ahead\n",
    "        wait_k = k * mean_r + (q_size - k) * mean_s\n",
    "        wait += prob * wait_k\n",
    "\n",
    "    return wait\n",
    "\n",
    "def rho(rate_regular, rate_specific, mean_r, mean_s):\n",
    "    \"\"\"\n",
    "    Calculate the traffic intensity (rho) for the queue.\n",
    "    \"\"\"\n",
    "    rate_total = rate_regular + rate_specific\n",
    "    mean_comb = (rate_regular / rate_total * mean_r) + (rate_specific / rate_total * mean_s)\n",
    "    \n",
    "    rhoo = rate_total * mean_comb\n",
    "    if rhoo >= 1:\n",
    "        return float('inf')  # Avoid division by zero or negative values\n",
    "    return rhoo\n",
    "\n",
    "def cv2(var, mean):\n",
    "    return var / (mean ** 2)\n",
    "\n",
    "def combined_ca2(rate_regular, rate_specific, ca_regular, ca_specific):\n",
    "    \"\"\"\n",
    "    Calculate the combined coefficient of variation squared for the arrival process.\n",
    "    \"\"\"\n",
    "    rate_total = rate_regular + rate_specific\n",
    "    \n",
    "    return (rate_regular / rate_total) * ca_regular + (rate_specific / rate_total) * ca_specific\n",
    "\n",
    "def combined_cs2(mean_regular, mean_specific, cs_regular, cs_specific):\n",
    "    \"\"\"\n",
    "    Calculate the combined coefficient of variation squared for the service process.\n",
    "    \"\"\"\n",
    "    rate_regular = 1 / mean_regular\n",
    "    rate_specific = 1 / mean_specific\n",
    "    rate_total = rate_regular + rate_specific\n",
    "    mean_comb = (rate_regular / rate_total * mean_regular) + (rate_specific / rate_total * mean_specific)\n",
    "    \n",
    "    term1 = (rate_regular / rate_total) * cs_regular\n",
    "    term2 = (rate_specific / rate_total) * cs_specific\n",
    "    term3 = rate_regular / rate_total * ((mean_regular - mean_comb) / mean_comb) ** 2\n",
    "    term4 = rate_specific / rate_total * ((mean_specific - mean_comb) / mean_comb) ** 2\n",
    "    \n",
    "    return term1 + term2 + term3 + term4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69235657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected waiting time at queue 1 (Kingman): 28384089.2543\n",
      "Expected waiting time at queue 2 (Kingman): 14287191.6796\n"
     ]
    }
   ],
   "source": [
    "# Example usage to calculate expected waiting time for queue 1:\n",
    "\n",
    "rate_regular = 1 / ARRIVAL_REGULAR\n",
    "rate_specific = 1 / ARRIVAL_SPECIFIC\n",
    "rate_total = rate_regular + rate_specific\n",
    "\n",
    "p_regular = rate_regular / rate_total\n",
    "\n",
    "mean_r1 = SERVICE_REGULAR_1\n",
    "mean_s1 = SERVICE_SPECIFIC_1\n",
    "\n",
    "rho1 = rho(rate_regular, rate_specific, mean_r1, mean_s1)\n",
    "\n",
    "meana_r = ARRIVAL_REGULAR\n",
    "meana_s = ARRIVAL_SPECIFIC\n",
    "vara_r = ARRIVAL_REGULAR_STD ** 2\n",
    "vara_s = ARRIVAL_SPECIFIC_STD ** 2\n",
    "\n",
    "ca_r = cv2(vara_r, meana_r)\n",
    "ca_s = cv2(vara_s, meana_s)\n",
    "\n",
    "vars_r1 = SERVICE_REGULAR_1_STD ** 2\n",
    "vars_s1 = SERVICE_SPECIFIC_1_STD ** 2\n",
    "\n",
    "cs_r1 = cv2(vars_r1, mean_r1)\n",
    "cs_s1 = cv2(vars_s1, mean_s1)\n",
    "\n",
    "ca = combined_ca2(rate_regular, rate_specific, ca_r, ca_s)\n",
    "cs_1 = combined_cs2(mean_r1, mean_s1, cs_r1, cs_s1)\n",
    "\n",
    "q_size = 10 # Example queue size\n",
    "\n",
    "expected_wait = expected_waiting_time_binom_kingman(q_size, p_regular, mean_r1, mean_s1, rho1, ca, cs_1)\n",
    "print(f\"Expected waiting time at queue 1 (Kingman): {expected_wait:.4f}\")\n",
    "\n",
    "# Compare with queue 2\n",
    "\n",
    "mean_r2 = SERVICE_REGULAR_2\n",
    "mean_s2 = SERVICE_SPECIFIC_2\n",
    "\n",
    "rho2 = rho(rate_regular, rate_specific, mean_r2, mean_s2)\n",
    "\n",
    "vars_r2 = SERVICE_REGULAR_2_STD ** 2\n",
    "vars_s2 = SERVICE_SPECIFIC_2_STD ** 2\n",
    "\n",
    "cs_r2 = cv2(vars_r2, mean_r2)\n",
    "cs_s2 = cv2(vars_s2, mean_s2)\n",
    "\n",
    "cs_2 = combined_cs2(mean_r2, mean_s2, cs_r2, cs_s2)\n",
    "\n",
    "expected_wait2 = expected_waiting_time_binom_kingman(q_size, p_regular, mean_r2, mean_s2, rho2, ca, cs_2)\n",
    "print(f\"Expected waiting time at queue 2 (Kingman): {expected_wait2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8443c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected waiting time at queue 1 (Binom): 130.9751\n",
      "Expected waiting time at queue 2 (Binom): 178.7353\n"
     ]
    }
   ],
   "source": [
    "expected_wait = expected_waiting_time_binom(q_size, p_regular, mean_r1, mean_s1)\n",
    "print(f\"Expected waiting time at queue 1 (Binom): {expected_wait:.4f}\")\n",
    "expected_wait2 = expected_waiting_time_binom(q_size, p_regular, mean_r2, mean_s2)\n",
    "print(f\"Expected waiting time at queue 2 (Binom): {expected_wait2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d5a86e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departure probability for REGULAR, Queue 1: 0.2606\n",
      "Departure probability for REGULAR, Queue 2: 0.1732\n",
      "Departure probability for SPECIFIC, Queue 1: 0.2315\n",
      "Departure probability for SPECIFIC, Queue 2: 0.2193\n"
     ]
    }
   ],
   "source": [
    "# Calculate departure probability\n",
    "r1_rate = 1 / mean_r1\n",
    "r2_rate = 1 / mean_r2\n",
    "s1_rate = 1 / mean_s1\n",
    "s2_rate = 1 / mean_s2\n",
    "\n",
    "total_rate = rate_regular + rate_specific + r1_rate + r2_rate + s1_rate + s2_rate\n",
    "\n",
    "print(f\"Departure probability for REGULAR, Queue 1: {r1_rate / total_rate:.4f}\")\n",
    "print(f\"Departure probability for REGULAR, Queue 2: {r2_rate / total_rate:.4f}\")\n",
    "print(f\"Departure probability for SPECIFIC, Queue 1: {s1_rate / total_rate:.4f}\")\n",
    "print(f\"Departure probability for SPECIFIC, Queue 2: {s2_rate / total_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b40281",
   "metadata": {},
   "source": [
    "### Use binomial expected waiting time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03245cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(costs, Q_table, max_queue_size):\n",
    "    \"\"\"Plot learning curve and learned policies.\"\"\"\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(costs, alpha=0.3, label=\"Episode Cost\")\n",
    "    window = 100\n",
    "    if len(costs) > window:\n",
    "        smoothed = pd.Series(costs).rolling(window).mean()\n",
    "        plt.plot(smoothed, color='red', linewidth=2, label=f\"Smoothed (window={window})\")\n",
    "    plt.title(\"Q-Learning: Cost vs Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learned policies\n",
    "    policy_regular = np.zeros((max_queue_size + 1, max_queue_size + 1), dtype=int)\n",
    "    policy_specific = np.zeros((max_queue_size + 1, max_queue_size + 1), dtype=int)\n",
    "\n",
    "    for q1 in range(max_queue_size + 1):\n",
    "        for q2 in range(max_queue_size + 1):\n",
    "            # Regular call policy\n",
    "            s_regular = (q1, q2, 0)\n",
    "            if s_regular in Q_table:\n",
    "                policy_regular[q1, q2] = np.argmax(Q_table[s_regular])\n",
    "            \n",
    "            # Specific call policy\n",
    "            s_specific = (q1, q2, 1)\n",
    "            if s_specific in Q_table:\n",
    "                policy_specific[q1, q2] = np.argmax(Q_table[s_specific])\n",
    "\n",
    "    # Plot policies\n",
    "    plt.subplot(1, 3, 2)\n",
    "    ax = sns.heatmap(np.flipud(policy_regular), cmap=\"RdYlBu\", annot=True, \n",
    "                     cbar_kws={'label': '0=Queue1, 1=Queue2'})\n",
    "    plt.title('Learned Policy: Regular Calls')\n",
    "    plt.ylabel('Queue 1 Length')\n",
    "    plt.xlabel('Queue 2 Length')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    ax = sns.heatmap(np.flipud(policy_specific), cmap=\"RdYlBu\", annot=True,\n",
    "                cbar_kws={'label': '0=Queue1, 1=Queue2'})\n",
    "    plt.title('Learned Policy: Specific Calls')\n",
    "    plt.ylabel('Queue 1 Length') \n",
    "    plt.xlabel('Queue 2 Length')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef37be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventBasedCallCentreEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Event-based RL environment for a two-queue call centre.\n",
    "    Each step represents a call arrival requiring a routing decision.\n",
    "    State: (queue1_length, queue2_length, call_type)\n",
    "    Action: 0 = route to queue 1, 1 = route to queue 2\n",
    "    Reward: Negative expected waiting time (cost to minimize)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_queue_size=10,\n",
    "                 drop_penalty=1000.0,\n",
    "                 arr_scaler = 1.0,\n",
    "                 arrival_regular_mean=None,\n",
    "                 arrival_specific_mean=None,\n",
    "                 service_regular_1=None,\n",
    "                 service_specific_1=None,\n",
    "                 service_regular_2=None,\n",
    "                 service_specific_2=None,\n",
    "                 seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the event-based call centre environment.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_queue_size = max_queue_size\n",
    "        self.drop_penalty = drop_penalty\n",
    "        self.arr_scaler = arr_scaler\n",
    "        \n",
    "        # Use provided parameters or defaults from your original code\n",
    "        self.arrival_params = {\n",
    "            'regular': {\n",
    "                'mean': arrival_regular_mean / self.arr_scaler,  # Default values\n",
    "                'mu': ARRIVAL_REGULAR_MU,      # These would come from your lognorm_params\n",
    "                'sigma': ARRIVAL_REGULAR_SIGMA,\n",
    "                'loc': ARRIVAL_REGULAR_LOC\n",
    "            },\n",
    "            'specific': {\n",
    "                'mean': arrival_specific_mean / self.arr_scaler,\n",
    "                'mu': ARRIVAL_SPECIFIC_MU,\n",
    "                'sigma': ARRIVAL_SPECIFIC_SIGMA,\n",
    "                'loc': ARRIVAL_SPECIFIC_LOC\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Service parameters\n",
    "        self.service_times = {\n",
    "            (1, 'regular'): service_regular_1,\n",
    "            (1, 'specific'): service_specific_1,\n",
    "            (2, 'regular'): service_regular_2,\n",
    "            (2, 'specific'): service_specific_2,\n",
    "        }\n",
    "        \n",
    "        # Calculate arrival rates and probabilities\n",
    "        arrival_regular_rate = 1.0 / self.arrival_params['regular']['mean']\n",
    "        arrival_specific_rate = 1.0 / self.arrival_params['specific']['mean']\n",
    "        self.total_arrival_rate = arrival_regular_rate + arrival_specific_rate\n",
    "        self.p_regular = arrival_regular_rate / self.total_arrival_rate\n",
    "        \n",
    "        # Define observation and action spaces\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0]), \n",
    "            high=np.array([max_queue_size, max_queue_size, 1]), \n",
    "            dtype=np.int32\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Initialize state\n",
    "        self.queue_lengths = [0, 0]\n",
    "        self.current_time = 0.0\n",
    "        self.episode_step = 0\n",
    "        \n",
    "        self.queue_contents = [[], []]  # Contents of each queue (list of tuples: (call_type, arrival_time))\n",
    "        \n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "            \n",
    "        # Initialize next arrival times for each call type\n",
    "        self.next_regular_arrival = 0.0\n",
    "        self.next_specific_arrival = 0.0\n",
    "        \n",
    "        self.current_call_type, _ = self._get_next_call_info()\n",
    "        \n",
    "        # Generate initial arrival times\n",
    "        self._generate_next_arrivals()\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            self.np_random = np.random.RandomState(seed)\n",
    "    \n",
    "    def _generate_lognormal_interarrival(self, call_type):\n",
    "        \"\"\"Generate next inter-arrival time using lognormal distribution.\"\"\"\n",
    "        params = self.arrival_params[call_type]\n",
    "        \n",
    "        # Generate from lognormal distribution\n",
    "        sample = np.random.lognormal(params['mu'], params['sigma'])\n",
    "        # Add location parameter (shift)\n",
    "        interarrival_time = sample + params['loc']\n",
    "        # Scale the interarrival\n",
    "        interarrival_time /= self.arr_scaler\n",
    "        \n",
    "        return max(interarrival_time, 0.1)\n",
    "    \n",
    "    def _generate_next_arrivals(self):\n",
    "        \"\"\"Generate next arrival times for both call types.\"\"\"\n",
    "        # Always generate next regular arrival if current one is used/passed\n",
    "        if self.next_regular_arrival <= self.current_time or self.next_regular_arrival == float('inf'):\n",
    "            regular_interarrival = self._generate_lognormal_interarrival('regular')\n",
    "            self.next_regular_arrival = self.current_time + regular_interarrival\n",
    "        \n",
    "        # Always generate next specific arrival if current one is used/passed  \n",
    "        if self.next_specific_arrival <= self.current_time or self.next_specific_arrival == float('inf'):\n",
    "            specific_interarrival = self._generate_lognormal_interarrival('specific')\n",
    "            self.next_specific_arrival = self.current_time + specific_interarrival\n",
    "\n",
    "    def _get_next_call_info(self):\n",
    "        \"\"\"Determine which call arrives next and when.\"\"\"\n",
    "        # Ensure we have valid next arrival times\n",
    "        self._generate_next_arrivals()\n",
    "        \n",
    "        if self.next_regular_arrival <= self.next_specific_arrival:\n",
    "            # Regular call arrives next\n",
    "            elapsed_time = self.next_regular_arrival - self.current_time\n",
    "            call_type = 0  # regular\n",
    "            # Mark this arrival as used\n",
    "            self.next_regular_arrival = float('inf')\n",
    "        else:\n",
    "            # Specific call arrives next\n",
    "            elapsed_time = self.next_specific_arrival - self.current_time\n",
    "            call_type = 1  # specific\n",
    "            # Mark this arrival as used\n",
    "            self.next_specific_arrival = float('inf')\n",
    "        \n",
    "        return call_type, elapsed_time\n",
    "\n",
    "    \n",
    "    def _process_service_completions(self, elapsed_time):\n",
    "        \"\"\"\n",
    "        Process service completions for two independent servers.\n",
    "        Each queue has its own server with different service times.\n",
    "        \"\"\"\n",
    "        for queue_idx in range(2):  # Two independent servers\n",
    "            if len(self.queue_contents[queue_idx]) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Process the first call in each queue (FIFO within each server)\n",
    "            call_type, arrival_time = self.queue_contents[queue_idx][0]\n",
    "            call_type_str = 'regular' if call_type == 0 else 'specific'\n",
    "            \n",
    "            # Time this call has been in the system\n",
    "            time_in_system = self.current_time - arrival_time + elapsed_time\n",
    "            \n",
    "            # Get service time for THIS specific server\n",
    "            mean_service_time = self.service_times[(queue_idx + 1, call_type_str)]\n",
    "            \n",
    "            # Exponential service completion check\n",
    "            service_rate = 1.0 / mean_service_time\n",
    "            completion_prob = 1.0 - np.exp(-service_rate * elapsed_time)  # Use elapsed_time, not cumulative\n",
    "            \n",
    "            if np.random.random() < completion_prob:\n",
    "                # Call completed at this server\n",
    "                self.queue_contents[queue_idx].pop(0)\n",
    "                self.queue_lengths[queue_idx] = len(self.queue_contents[queue_idx])\n",
    "    \n",
    "    def expected_waiting_time_binomial(self, queue_idx, call_type):\n",
    "        \"\"\"\n",
    "        Calculate expected waiting time using binomial mixture of call types.\n",
    "        This is the same calculation as in your original code.\n",
    "        \"\"\"\n",
    "        queue_size = self.queue_lengths[queue_idx]\n",
    "        \n",
    "        if queue_size == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Service times for this queue\n",
    "        mean_regular = self.service_times[(queue_idx + 1, 'regular')]\n",
    "        mean_specific = self.service_times[(queue_idx + 1, 'specific')]\n",
    "        \n",
    "        # Calculate expected waiting time using binomial mixture\n",
    "        expected_wait = 0.0\n",
    "        for k in range(queue_size + 1):\n",
    "            prob = comb(queue_size, k) * (self.p_regular ** k) * ((1 - self.p_regular) ** (queue_size - k))\n",
    "            wait_time = k * mean_regular + (queue_size - k) * mean_specific\n",
    "            expected_wait += prob * wait_time\n",
    "        \n",
    "        return expected_wait\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        \n",
    "        self.queue_lengths = [0, 0]\n",
    "        self.queue_contents = [[], []]\n",
    "        self.current_time = 0.0\n",
    "        self.episode_step = 0\n",
    "        \n",
    "        # Reset arrival times\n",
    "        self.next_regular_arrival = 0.0\n",
    "        self.next_specific_arrival = 0.0\n",
    "        self._generate_next_arrivals()\n",
    "        \n",
    "        self.current_call_type, _ = self._get_next_call_info()\n",
    "        \n",
    "        return self._get_state(self.current_call_type), {}\n",
    "    \n",
    "    def _get_state(self, call_type):\n",
    "        \"\"\"Get current state as numpy array.\"\"\"\n",
    "        return np.array([\n",
    "            self.queue_lengths[0], \n",
    "            self.queue_lengths[1], \n",
    "            call_type\n",
    "        ], dtype=np.int32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action and return next state, reward, done, info.\"\"\"\n",
    "        # Use the current call that was determined in reset() or previous step\n",
    "        call_type = self.current_call_type\n",
    "        call_type_str = 'regular' if call_type == 0 else 'specific'\n",
    "        \n",
    "        # Get the elapsed time for this call\n",
    "        _, elapsed_time = self._get_current_call_timing()\n",
    "        \n",
    "        # Update current time\n",
    "        self.current_time += elapsed_time\n",
    "        \n",
    "        # Process service completions during this inter-arrival time\n",
    "        self._process_service_completions(elapsed_time)\n",
    "        \n",
    "        reward = 0.0\n",
    "        dropped = False\n",
    "        expected_wait = 0.0\n",
    "        \n",
    "        # Check if queue is full\n",
    "        if self.queue_lengths[action] >= self.max_queue_size:\n",
    "            reward = -self.drop_penalty\n",
    "            dropped = True\n",
    "            expected_wait = None\n",
    "        else:\n",
    "            # Calculate expected waiting time for this call\n",
    "            expected_wait = self.expected_waiting_time_binomial(action, call_type)\n",
    "            reward = -expected_wait\n",
    "            # Add call to chosen queue\n",
    "            self.queue_contents[action].append((call_type, self.current_time))\n",
    "            self.queue_lengths[action] += 1\n",
    "        \n",
    "        # Update step counter\n",
    "        self.episode_step += 1\n",
    "        \n",
    "        # NOW generate the NEXT call for the next step\n",
    "        self.current_call_type, _ = self._get_next_call_info()\n",
    "        \n",
    "        # Determine if episode should continue\n",
    "        terminated = False\n",
    "        truncated = self.episode_step >= 1000  # Max episode length\n",
    "        \n",
    "        info = {\n",
    "            'queue_1_length': self.queue_lengths[0],\n",
    "            'queue_2_length': self.queue_lengths[1],\n",
    "            'call_type': call_type_str,\n",
    "            'chosen_queue': action + 1 if not dropped else None,\n",
    "            'expected_wait': expected_wait,\n",
    "            'dropped': dropped,\n",
    "            'reward': reward,\n",
    "            'current_time': self.current_time,\n",
    "            'episode_step': self.episode_step,\n",
    "            'elapsed_time': elapsed_time,\n",
    "            'next_call_type': 'regular' if self.current_call_type == 0 else 'specific',\n",
    "            'queue_1_contents': len(self.queue_contents[0]),\n",
    "            'queue_2_contents': len(self.queue_contents[1]),\n",
    "            'next_regular_arrival': self.next_regular_arrival,\n",
    "            'next_specific_arrival': self.next_specific_arrival\n",
    "        }\n",
    "        \n",
    "        return self._get_state(self.current_call_type), reward, terminated, truncated, info\n",
    "    \n",
    "    def _get_current_call_timing(self):\n",
    "        \"\"\"Get timing for the current call without consuming arrival streams.\"\"\"\n",
    "        self._generate_next_arrivals()\n",
    "        \n",
    "        if self.current_call_type == 0:  # regular\n",
    "            elapsed_time = self.next_regular_arrival - self.current_time\n",
    "            # Mark this arrival as used\n",
    "            self.next_regular_arrival = float('inf')\n",
    "        else:  # specific\n",
    "            elapsed_time = self.next_specific_arrival - self.current_time\n",
    "            # Mark this arrival as used\n",
    "            self.next_specific_arrival = float('inf')\n",
    "        \n",
    "        return self.current_call_type, max(elapsed_time, 0.01)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render current state.\"\"\"\n",
    "        if hasattr(self, '_current_call_type'):\n",
    "            call_type_str = 'Regular' if self._current_call_type == 0 else 'Specific'\n",
    "        else:\n",
    "            call_type_str = 'Unknown'\n",
    "        \n",
    "        print(f\"Step {self.episode_step} | Time: {self.current_time:.1f}s | Queue 1: {self.queue_lengths[0]:2d} | Queue 2: {self.queue_lengths[1]:2d} | Current Call: {call_type_str}\")\n",
    "    \n",
    "    def get_state_info(self):\n",
    "        \"\"\"Get detailed state information for analysis.\"\"\"\n",
    "        call_type = self.current_call_type\n",
    "        call_type_str = 'Regular' if call_type == 0 else 'Specific'\n",
    "        \n",
    "        wait_q1 = self.expected_waiting_time_binomial(0, call_type)\n",
    "        wait_q2 = self.expected_waiting_time_binomial(1, call_type)\n",
    "        \n",
    "        return {\n",
    "            'queue_1_length': self.queue_lengths[0],\n",
    "            'queue_2_length': self.queue_lengths[1],\n",
    "            'call_type': call_type_str,\n",
    "            'p_regular': self.p_regular,\n",
    "            'expected_wait_q1': wait_q1,\n",
    "            'expected_wait_q2': wait_q2,\n",
    "            'optimal_choice': 1 if wait_q1 <= wait_q2 else 2,\n",
    "            'current_time': self.current_time,\n",
    "            'episode_step': self.episode_step\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e61b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the environment\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Call Centre MDP Environment\")\n",
    "    \n",
    "#     # Set parameters for the environment\n",
    "#     max_queue_size = MAX_QUEUE_SIZE\n",
    "#     drop_penalty = DROP_PENALTY\n",
    "#     scaler = 10.0\n",
    "\n",
    "#     # Arrival parameters\n",
    "#     arrival_means = {\n",
    "#         0: ARRIVAL_REGULAR,    # Regular calls\n",
    "#         1: ARRIVAL_SPECIFIC    # Specific calls\n",
    "#     }\n",
    "    \n",
    "#     # Service parameters\n",
    "#     service_means = {\n",
    "#         (1, 0): SERVICE_REGULAR_1,\n",
    "#         (1, 1): SERVICE_SPECIFIC_1,\n",
    "#         (2, 0): SERVICE_REGULAR_2,\n",
    "#         (2, 1): SERVICE_SPECIFIC_2\n",
    "#     }\n",
    "\n",
    "#     print(f\"Regular call probability: {p_regular:.3f}\")\n",
    "#     print(f\"Service times (mean): Q1 Regular = {service_means[(1,0)]:.1f} s, Specific = {service_means[(1,1)]:.1f} s\")\n",
    "#     print(f\"                     Q2 Regular = {service_means[(2,0)]:.1f} s, Specific = {service_means[(2,1)]:.1f} s\")\n",
    "#     print(f\"Arrival times (mean inter-arrival time): Regular = {arrival_means[0] / scaler:.1f} s, Specific = {arrival_means[1] / scaler:.1f} s\")\n",
    "#     print(\"=\"*60)\n",
    "\n",
    "#     # Set random seeds for reproducibility\n",
    "#     random.seed(1901448)\n",
    "#     np.random.seed(1901448)\n",
    "    \n",
    "#     # Create environment with your original parameters\n",
    "#     env = EventBasedCallCentreEnv(\n",
    "#         max_queue_size=max_queue_size,\n",
    "#         drop_penalty=drop_penalty,\n",
    "#         arr_scaler=scaler,\n",
    "#         arrival_regular_mean=arrival_means[0],  # Your ARRIVAL_REGULAR\n",
    "#         arrival_specific_mean=arrival_means[1],  # Your ARRIVAL_SPECIFIC\n",
    "#         service_regular_1=service_means[(1, 0)],     # Your SERVICE_REGULAR_1\n",
    "#         service_specific_1=service_means[(1, 1)],    # Your SERVICE_SPECIFIC_1\n",
    "#         service_regular_2=service_means[(2, 0)],     # Your SERVICE_REGULAR_2\n",
    "#         service_specific_2=service_means[(2, 1)],    # Your SERVICE_SPECIFIC_2\n",
    "#         seed=1901448\n",
    "#     )\n",
    "    \n",
    "#     print(\"Testing Event-Based Call Centre Environment\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     # Test environment\n",
    "#     state, _ = env.reset()\n",
    "#     print(f\"Initial state: Queue1={state[0]}, Queue2={state[1]}, CallType={'Regular' if state[2]==0 else 'Specific'}\")\n",
    "    \n",
    "#     # Run a few test steps\n",
    "#     total_reward = 0\n",
    "#     for step in range(1000):\n",
    "#         # Get state information\n",
    "#         info = env.get_state_info()\n",
    "        \n",
    "#         # Choose action (0 = Queue 1, 1 = Queue 2)\n",
    "#         # Simple policy: choose queue with shorter expected wait\n",
    "#         if info.get('expected_wait_q1', float('inf')) <= info.get('expected_wait_q2', float('inf')):\n",
    "#             action = 0\n",
    "#         else:\n",
    "#             action = 1\n",
    "        \n",
    "#         # Take step\n",
    "#         next_state, reward, terminated, truncated, step_info = env.step(action)\n",
    "#         total_reward += reward\n",
    "        \n",
    "#         print(f\"Step {step+1}:\")\n",
    "#         print(f\"  Action: Queue {action+1}\")\n",
    "#         print(f\"  Call Type: {step_info['call_type']}\")\n",
    "#         print(f\"  Expected Wait: {step_info['expected_wait']:.1f}s\" if step_info['expected_wait'] is not None else \"  DROPPED\")\n",
    "#         print(f\"  New State: Queue1={next_state[0]}, Queue2={next_state[1]}\")\n",
    "#         print(f\"  Elapsed Time: {step_info['elapsed_time']:.1f}s\")\n",
    "#         print(f\"  Reward: {reward:.1f}\")\n",
    "#         print()\n",
    "        \n",
    "#         if terminated or truncated:\n",
    "#             break\n",
    "    \n",
    "#     print(f\"Total Reward: {total_reward:.1f}\")\n",
    "#     print(f\"Final Time: {step_info['current_time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e39730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_queue_baseline(env, num_episodes=100):\n",
    "    \"\"\"Baseline: always choose the shorter queue.\"\"\"\n",
    "    total_costs = []\n",
    "    dropped_calls = 0\n",
    "    total_calls = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_cost = 0\n",
    "\n",
    "        for step in range(200):\n",
    "            q1, q2, call_type = state\n",
    "\n",
    "            # Choose shorter queue, with tie-breaking\n",
    "            if q1 < q2:\n",
    "                action = 0  # queue 1\n",
    "            elif q2 < q1:\n",
    "                action = 1  # queue 2\n",
    "            else:\n",
    "                # Tie-break: regular calls to queue 1, specific to queue 2\n",
    "                action = 0 if call_type == 0 else 1\n",
    "\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_cost += -reward\n",
    "            total_calls += 1\n",
    "\n",
    "            if info.get(\"dropped\", False):\n",
    "                dropped_calls += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        total_costs.append(episode_cost)\n",
    "\n",
    "    avg_cost = np.mean(total_costs)\n",
    "    std_cost = np.std(total_costs)\n",
    "    drop_rate = dropped_calls / total_calls if total_calls > 0 else 0\n",
    "\n",
    "    print(\"Shortest Queue Baseline:\")\n",
    "    print(f\"  Average cost per episode: {avg_cost:.2f} ± {std_cost:.2f}\")\n",
    "    print(f\"  Drop rate: {drop_rate:.4f} ({dropped_calls}/{total_calls})\")\n",
    "\n",
    "    return avg_cost, std_cost, drop_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3aba0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedQLearning:\n",
    "    \"\"\"\n",
    "    Advanced Q-Learning with problem-specific improvements for call center routing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 initial_alpha=0.5,\n",
    "                 min_alpha=0.01,\n",
    "                 gamma=0.99,\n",
    "                 initial_epsilon=1.0,\n",
    "                 min_epsilon=0.02,\n",
    "                 epsilon_decay=0.999,\n",
    "                 use_prioritized_sweeping=True,\n",
    "                 use_experience_replay=True,\n",
    "                 replay_buffer_size=10000,\n",
    "                 batch_size=32,\n",
    "                 target_update_freq=1000,\n",
    "                 exploration_strategy='combined'):\n",
    "        \n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.initial_alpha = initial_alpha\n",
    "        self.min_alpha = min_alpha\n",
    "        self.current_alpha = initial_alpha\n",
    "        self.initial_epsilon = initial_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.current_epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Advanced features\n",
    "        self.use_prioritized_sweeping = use_prioritized_sweeping\n",
    "        self.use_experience_replay = use_experience_replay\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.exploration_strategy = exploration_strategy\n",
    "        \n",
    "        # Q-tables: main and target (for stability)\n",
    "        self.Q = defaultdict(lambda: np.zeros(2))\n",
    "        self.Q_target = defaultdict(lambda: np.zeros(2))\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.experience_buffer = deque(maxlen=replay_buffer_size)\n",
    "        \n",
    "        # State-action visit counts and recency\n",
    "        self.visit_counts = defaultdict(lambda: np.zeros(2))\n",
    "        self.last_visited = defaultdict(lambda: np.zeros(2))\n",
    "        self.state_visits = defaultdict(int)\n",
    "        \n",
    "        # Priority queue for prioritized sweeping\n",
    "        self.priority_queue = {}\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.episode_rewards = []\n",
    "        self.episode_costs = []\n",
    "        self.epsilon_history = []\n",
    "        self.alpha_history = []\n",
    "        self.td_errors = []\n",
    "        self.exploration_stats = {'random': 0, 'greedy': 0, 'ucb': 0, 'count_based': 0}\n",
    "        \n",
    "        # Problem-specific parameters\n",
    "        self.drop_penalty_factor = 2.0  # Extra penalty for dropping calls\n",
    "        self.balance_bonus = 10.0  # Bonus for keeping queues balanced\n",
    "        \n",
    "    def get_exploration_bonus(self, state, action, episode):\n",
    "        \"\"\"Calculate exploration bonus based on visit count and recency.\"\"\"\n",
    "        state_tuple = tuple(state)\n",
    "        visits = self.visit_counts[state_tuple][action]\n",
    "        \n",
    "        if visits == 0:\n",
    "            return 1000.0  # High bonus for never-visited state-actions\n",
    "        \n",
    "        # Count-based exploration bonus\n",
    "        count_bonus = 100.0 / np.sqrt(visits + 1)\n",
    "        \n",
    "        # Recency bonus (encourage revisiting old states)\n",
    "        episodes_since_last = episode - self.last_visited[state_tuple][action]\n",
    "        recency_bonus = min(50.0, episodes_since_last * 0.1)\n",
    "        \n",
    "        return count_bonus + recency_bonus\n",
    "    \n",
    "    def select_action(self, state, episode):\n",
    "        \"\"\"Advanced action selection with multiple exploration strategies.\"\"\"\n",
    "        state_tuple = tuple(state)\n",
    "        q1, q2, call_type = state\n",
    "        \n",
    "        # Problem-specific heuristics\n",
    "        if q1 >= self.env.max_queue_size and q2 >= self.env.max_queue_size:\n",
    "            # Both queues full - this should rarely happen with good policy\n",
    "            return 0  # Default to queue 1\n",
    "        elif q1 >= self.env.max_queue_size:\n",
    "            return 1  # Must choose queue 2\n",
    "        elif q2 >= self.env.max_queue_size:\n",
    "            return 0  # Must choose queue 1\n",
    "        \n",
    "        # Exploration strategies\n",
    "        if np.random.random() < self.current_epsilon:\n",
    "            if self.exploration_strategy == 'combined':\n",
    "                # 50% random, 50% count-based exploration\n",
    "                if np.random.random() < 0.5:\n",
    "                    action = np.random.randint(0, 2)\n",
    "                    self.exploration_stats['random'] += 1\n",
    "                else:\n",
    "                    # Count-based: choose less visited action\n",
    "                    visits = self.visit_counts[state_tuple]\n",
    "                    if visits[0] <= visits[1]:\n",
    "                        action = 0\n",
    "                    else:\n",
    "                        action = 1\n",
    "                    self.exploration_stats['count_based'] += 1\n",
    "            else:\n",
    "                action = np.random.randint(0, 2)\n",
    "                self.exploration_stats['random'] += 1\n",
    "        else:\n",
    "            # Exploitation with exploration bonuses\n",
    "            q_values = self.Q[state_tuple].copy()\n",
    "            \n",
    "            # Add exploration bonuses\n",
    "            for a in range(2):\n",
    "                q_values[a] += self.get_exploration_bonus(state, a, episode) * 0.01\n",
    "            \n",
    "            # Add problem-specific bonuses\n",
    "            # Bonus for queue balancing\n",
    "            queue_diff = abs(q1 - q2)\n",
    "            if queue_diff > 3:  # Significant imbalance\n",
    "                if q1 > q2:\n",
    "                    q_values[1] += self.balance_bonus  # Prefer queue 2\n",
    "                else:\n",
    "                    q_values[0] += self.balance_bonus  # Prefer queue 1\n",
    "            \n",
    "            # Call type specific bonuses based on service times\n",
    "            if call_type == 0:  # Regular calls\n",
    "                # Queue 1 is faster for regular calls\n",
    "                q_values[0] += 20.0\n",
    "            else:  # Specific calls\n",
    "                # Queue 2 might be better for specific calls in some cases\n",
    "                if q2 < q1:  # If queue 2 is shorter\n",
    "                    q_values[1] += 15.0\n",
    "            \n",
    "            action = int(np.argmax(q_values))\n",
    "            self.exploration_stats['greedy'] += 1\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience for replay.\"\"\"\n",
    "        experience = (tuple(state), action, reward, tuple(next_state), done)\n",
    "        self.experience_buffer.append(experience)\n",
    "    \n",
    "    def sample_experience_batch(self):\n",
    "        \"\"\"Sample a batch of experiences for learning.\"\"\"\n",
    "        if len(self.experience_buffer) < self.batch_size:\n",
    "            return list(self.experience_buffer)\n",
    "        return random.sample(self.experience_buffer, self.batch_size)\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, episode, done=False):\n",
    "        \"\"\"Enhanced Q-value update with experience replay and target network.\"\"\"\n",
    "        state_tuple = tuple(state)\n",
    "        next_state_tuple = tuple(next_state)\n",
    "        \n",
    "        # Adaptive learning rate based on visit count\n",
    "        visits = self.visit_counts[state_tuple][action]\n",
    "        adaptive_alpha = max(self.min_alpha, self.initial_alpha / (1 + visits * 0.01))\n",
    "        \n",
    "        # Modified reward for better learning\n",
    "        modified_reward = reward\n",
    "        \n",
    "        # Extra penalty for drops\n",
    "        if reward <= -self.env.drop_penalty * 0.8:  # Dropped call\n",
    "            modified_reward *= self.drop_penalty_factor\n",
    "        \n",
    "        # Standard Q-learning update using target network\n",
    "        current_q = self.Q[state_tuple][action]\n",
    "        \n",
    "        if done:\n",
    "            target_q = modified_reward\n",
    "        else:\n",
    "            # Use target network for stability\n",
    "            next_q_max = np.max(self.Q_target[next_state_tuple])\n",
    "            target_q = modified_reward + self.gamma * next_q_max\n",
    "        \n",
    "        td_error = target_q - current_q\n",
    "        self.Q[state_tuple][action] += adaptive_alpha * td_error\n",
    "        \n",
    "        # Update visit counts and recency\n",
    "        self.visit_counts[state_tuple][action] += 1\n",
    "        self.last_visited[state_tuple][action] = episode\n",
    "        self.state_visits[state_tuple] += 1\n",
    "        \n",
    "        return abs(td_error)\n",
    "    \n",
    "    def experience_replay(self, episode):\n",
    "        \"\"\"Perform experience replay learning.\"\"\"\n",
    "        if not self.use_experience_replay or len(self.experience_buffer) < self.batch_size:\n",
    "            return []\n",
    "        \n",
    "        batch = self.sample_experience_batch()\n",
    "        td_errors = []\n",
    "        \n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            td_error = self.update_q_value(\n",
    "                state, action, reward, next_state, episode, done\n",
    "            )\n",
    "            td_errors.append(td_error)\n",
    "        \n",
    "        return td_errors\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network.\"\"\"\n",
    "        for state_tuple in self.Q:\n",
    "            self.Q_target[state_tuple] = self.Q[state_tuple].copy()\n",
    "    \n",
    "    def decay_parameters(self, episode):\n",
    "        \"\"\"Update exploration and learning parameters.\"\"\"\n",
    "        # More aggressive epsilon decay early, then slow down\n",
    "        if episode < 5000:\n",
    "            decay_rate = 0.9995\n",
    "        elif episode < 15000:\n",
    "            decay_rate = 0.999\n",
    "        else:\n",
    "            decay_rate = 0.9999\n",
    "            \n",
    "        self.current_epsilon = max(\n",
    "            self.min_epsilon, \n",
    "            self.current_epsilon * decay_rate\n",
    "        )\n",
    "        \n",
    "        # Learning rate decay\n",
    "        self.current_alpha = max(\n",
    "            self.min_alpha,\n",
    "            self.current_alpha * 0.9999\n",
    "        )\n",
    "        \n",
    "        self.epsilon_history.append(self.current_epsilon)\n",
    "        self.alpha_history.append(self.current_alpha)\n",
    "    \n",
    "    def train(self, num_episodes=50000, max_steps_per_episode=200, \n",
    "              print_every=5000, warmup_episodes=1000):\n",
    "        \"\"\"Enhanced training loop with warmup period.\"\"\"\n",
    "        print(\"Starting Advanced Q-Learning Training...\")\n",
    "        print(f\"Episodes: {num_episodes}, Warmup: {warmup_episodes}\")\n",
    "        print(f\"Experience replay: {self.use_experience_replay}\")\n",
    "        print(f\"Target network updates every: {self.target_update_freq} episodes\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        recent_rewards = deque(maxlen=1000)\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_td_errors = []\n",
    "            \n",
    "            # Warmup phase: more exploration\n",
    "            if episode < warmup_episodes:\n",
    "                self.current_epsilon = max(0.5, self.current_epsilon)\n",
    "            \n",
    "            for step in range(max_steps_per_episode):\n",
    "                action = self.select_action(state, episode)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                \n",
    "                # Store experience\n",
    "                self.store_experience(state, action, reward, next_state, \n",
    "                                    terminated or truncated)\n",
    "                \n",
    "                # Update Q-value\n",
    "                td_error = self.update_q_value(state, action, reward, next_state, \n",
    "                                             episode, terminated or truncated)\n",
    "                episode_td_errors.append(td_error)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Experience replay\n",
    "            if episode > warmup_episodes:\n",
    "                replay_td_errors = self.experience_replay(episode)\n",
    "                episode_td_errors.extend(replay_td_errors)\n",
    "            \n",
    "            # Update target network\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.update_target_network()\n",
    "            \n",
    "            # Store episode statistics\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_costs.append(-episode_reward)\n",
    "            recent_rewards.append(episode_reward)\n",
    "            \n",
    "            if episode_td_errors:\n",
    "                self.td_errors.append(np.mean(episode_td_errors))\n",
    "            \n",
    "            # Parameter decay\n",
    "            self.decay_parameters(episode)\n",
    "            \n",
    "            # Progress reporting\n",
    "            if (episode + 1) % print_every == 0:\n",
    "                avg_reward = np.mean(recent_rewards) if recent_rewards else 0\n",
    "                avg_cost = -avg_reward\n",
    "                avg_td_error = np.mean(self.td_errors[-print_every:]) if self.td_errors else 0\n",
    "                states_explored = len(self.Q)\n",
    "                \n",
    "                print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "                print(f\"  Avg Cost (last {len(recent_rewards)}): {avg_cost:.2f}\")\n",
    "                print(f\"  Avg TD Error: {avg_td_error:.4f}\")\n",
    "                print(f\"  Epsilon: {self.current_epsilon:.4f}\")\n",
    "                print(f\"  Alpha: {self.current_alpha:.4f}\")\n",
    "                print(f\"  States explored: {states_explored}\")\n",
    "                print(f\"  Experience buffer: {len(self.experience_buffer)}\")\n",
    "                \n",
    "                # Exploration statistics\n",
    "                total_actions = sum(self.exploration_stats.values())\n",
    "                if total_actions > 0:\n",
    "                    print(\"  Exploration breakdown:\")\n",
    "                    for strategy, count in self.exploration_stats.items():\n",
    "                        pct = (count / total_actions) * 100\n",
    "                        print(f\"    {strategy}: {pct:.1f}%\")\n",
    "                print()\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return self.Q, self.episode_costs\n",
    "    \n",
    "    def get_policy_analysis(self, max_queue_size=20):\n",
    "        \"\"\"Analyze the learned policy in detail.\"\"\"\n",
    "        policy_regular = np.zeros((max_queue_size + 1, max_queue_size + 1), dtype=int)\n",
    "        policy_specific = np.zeros((max_queue_size + 1, max_queue_size + 1), dtype=int)\n",
    "        q_value_diff = np.zeros((max_queue_size + 1, max_queue_size + 1))\n",
    "        \n",
    "        analysis = {\n",
    "            'total_states': 0,\n",
    "            'states_learned': 0,\n",
    "            'avg_q_value_diff': 0,\n",
    "            'strong_preferences': 0\n",
    "        }\n",
    "        \n",
    "        for q1 in range(max_queue_size + 1):\n",
    "            for q2 in range(max_queue_size + 1):\n",
    "                analysis['total_states'] += 1\n",
    "                \n",
    "                # Regular call policy\n",
    "                state_regular = (q1, q2, 0)\n",
    "                if state_regular in self.Q:\n",
    "                    analysis['states_learned'] += 1\n",
    "                    q_values = self.Q[state_regular]\n",
    "                    policy_regular[q1, q2] = np.argmax(q_values)\n",
    "                    diff = abs(q_values[0] - q_values[1])\n",
    "                    q_value_diff[q1, q2] = diff\n",
    "                    \n",
    "                    if diff > 100:  # Strong preference\n",
    "                        analysis['strong_preferences'] += 1\n",
    "                \n",
    "                # Specific call policy\n",
    "                state_specific = (q1, q2, 1)\n",
    "                if state_specific in self.Q:\n",
    "                    q_values = self.Q[state_specific]\n",
    "                    policy_specific[q1, q2] = np.argmax(q_values)\n",
    "        \n",
    "        if analysis['states_learned'] > 0:\n",
    "            analysis['coverage'] = analysis['states_learned'] / analysis['total_states']\n",
    "            analysis['avg_q_value_diff'] = np.mean(q_value_diff[q_value_diff > 0])\n",
    "        \n",
    "        return policy_regular, policy_specific, q_value_diff, analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bb5768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_advanced_qlearning(env, num_episodes=60000):\n",
    "    \"\"\"Run the advanced Q-learning algorithm with extended training.\"\"\"\n",
    "    \n",
    "    agent = AdvancedQLearning(\n",
    "        env=env,\n",
    "        initial_alpha=0.3,\n",
    "        min_alpha=0.005,\n",
    "        gamma=0.99,  # Higher discount for longer-term planning\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.01,\n",
    "        epsilon_decay=0.999,\n",
    "        use_prioritized_sweeping=False,  # Keep it simple for now\n",
    "        use_experience_replay=True,\n",
    "        replay_buffer_size=50000,  # Larger buffer\n",
    "        batch_size=64,  # Larger batch\n",
    "        target_update_freq=500,  # More frequent updates\n",
    "        exploration_strategy='combined'\n",
    "    )\n",
    "    \n",
    "    # Extended training\n",
    "    Q_table, episode_costs = agent.train(\n",
    "        num_episodes=num_episodes,\n",
    "        max_steps_per_episode=300,  # Longer episodes\n",
    "        print_every=5000,\n",
    "        warmup_episodes=2000  # Longer warmup\n",
    "    )\n",
    "    \n",
    "    # Analyze the learned policy\n",
    "    policy_reg, policy_spec, q_diff, analysis = agent.get_policy_analysis()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"POLICY ANALYSIS:\")\n",
    "    print(f\"State space coverage: {analysis['coverage']:.2%}\")\n",
    "    print(f\"States with strong preferences: {analysis['strong_preferences']}\")\n",
    "    print(f\"Average Q-value difference: {analysis.get('avg_q_value_diff', 0):.2f}\")\n",
    "    \n",
    "    return agent, Q_table, episode_costs\n",
    "\n",
    "\n",
    "# Enhanced evaluation function\n",
    "def advanced_evaluation(env, Q_table, num_episodes=1000, policy_name=\"Advanced Q-Learning\"):\n",
    "    \"\"\"Comprehensive policy evaluation.\"\"\"\n",
    "    total_costs = []\n",
    "    dropped_calls = 0\n",
    "    total_calls = 0\n",
    "    queue_stats = {'q1_lengths': [], 'q2_lengths': [], 'total_lengths': []}\n",
    "    action_stats = {'regular_to_q1': 0, 'regular_to_q2': 0, \n",
    "                   'specific_to_q1': 0, 'specific_to_q2': 0}\n",
    "    wait_time_stats = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_cost = 0\n",
    "        \n",
    "        for step in range(300):  # Longer evaluation episodes\n",
    "            state_tuple = tuple(state)\n",
    "            q1, q2, call_type = state\n",
    "            \n",
    "            queue_stats['q1_lengths'].append(q1)\n",
    "            queue_stats['q2_lengths'].append(q2)\n",
    "            queue_stats['total_lengths'].append(q1 + q2)\n",
    "            \n",
    "            # Policy decision\n",
    "            if state_tuple in Q_table:\n",
    "                q_values = Q_table[state_tuple]\n",
    "                \n",
    "                # Add small amount of intelligent tie-breaking\n",
    "                if abs(q_values[0] - q_values[1]) < 1:\n",
    "                    # Use domain knowledge for ties\n",
    "                    if call_type == 0 and q1 <= q2:  # Regular calls prefer Q1 if not worse\n",
    "                        action = 0\n",
    "                    elif call_type == 1 and q2 <= q1:  # Specific calls can prefer Q2\n",
    "                        action = 1\n",
    "                    else:\n",
    "                        action = 0 if q1 < q2 else 1\n",
    "                else:\n",
    "                    action = int(np.argmax(q_values))\n",
    "            else:\n",
    "                # Default policy for unvisited states\n",
    "                if q1 < q2:\n",
    "                    action = 0\n",
    "                elif q2 < q1:\n",
    "                    action = 1\n",
    "                else:\n",
    "                    action = 0 if call_type == 0 else 1\n",
    "            \n",
    "            # Track actions\n",
    "            if call_type == 0:\n",
    "                if action == 0:\n",
    "                    action_stats['regular_to_q1'] += 1\n",
    "                else:\n",
    "                    action_stats['regular_to_q2'] += 1\n",
    "            else:\n",
    "                if action == 0:\n",
    "                    action_stats['specific_to_q1'] += 1\n",
    "                else:\n",
    "                    action_stats['specific_to_q2'] += 1\n",
    "            \n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_cost += -reward\n",
    "            total_calls += 1\n",
    "            \n",
    "            if info.get(\"dropped\", False):\n",
    "                dropped_calls += 1\n",
    "            elif info.get(\"expected_wait\") is not None:\n",
    "                wait_time_stats.append(info[\"expected_wait\"])\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        total_costs.append(episode_cost)\n",
    "    \n",
    "    # Calculate comprehensive statistics\n",
    "    avg_cost = np.mean(total_costs)\n",
    "    std_cost = np.std(total_costs)\n",
    "    drop_rate = dropped_calls / total_calls if total_calls > 0 else 0\n",
    "    \n",
    "    avg_wait = np.mean(wait_time_stats) if wait_time_stats else 0\n",
    "    avg_q1_util = np.mean(queue_stats['q1_lengths'])\n",
    "    avg_q2_util = np.mean(queue_stats['q2_lengths'])\n",
    "    avg_total_util = np.mean(queue_stats['total_lengths'])\n",
    "    \n",
    "    print(f\"\\n{policy_name} Comprehensive Evaluation:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Average cost per episode:    {avg_cost:.2f} ± {std_cost:.2f}\")\n",
    "    print(f\"Drop rate:                   {drop_rate:.4f} ({dropped_calls}/{total_calls})\")\n",
    "    print(f\"Average waiting time:        {avg_wait:.2f}s\")\n",
    "    print(\"Queue utilization:\")\n",
    "    print(f\"  Queue 1 avg length:        {avg_q1_util:.2f}\")\n",
    "    print(f\"  Queue 2 avg length:        {avg_q2_util:.2f}\")\n",
    "    print(f\"  Total system load:         {avg_total_util:.2f}\")\n",
    "    \n",
    "    print(\"Action distribution:\")\n",
    "    total_actions = sum(action_stats.values())\n",
    "    for action_type, count in action_stats.items():\n",
    "        percentage = (count / total_actions) * 100 if total_actions > 0 else 0\n",
    "        print(f\"  {action_type.replace('_', ' ').title()}: {percentage:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'avg_cost': avg_cost,\n",
    "        'std_cost': std_cost,\n",
    "        'drop_rate': drop_rate,\n",
    "        'avg_wait': avg_wait,\n",
    "        'queue_util': (avg_q1_util, avg_q2_util),\n",
    "        'action_distribution': action_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb1257f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Q-Learning for Call Centre Environment\n",
      "============================================================\n",
      "Regular call probability: 0.713\n",
      "Service times (mean): Q1 Regular = 12.6 s, Specific = 14.2 s\n",
      "                     Q2 Regular = 19.0 s, Specific = 15.0 s\n",
      "Arrival times (mean): Regular = 4.0 s, Specific = 9.9 s\n",
      "============================================================\n",
      "\n",
      "1. BASELINE EVALUATION:\n",
      "Shortest Queue Baseline:\n",
      "  Average cost per episode: 145797.99 ± 32213.89\n",
      "  Drop rate: 0.1578 (31550/200000)\n",
      "\n",
      "3. ENHANCED Q-LEARNING:\n",
      "Training enhanced Q-learning...\n",
      "Advanced Q-Learning with Extended Training\n",
      "============================================================\n",
      "Starting Advanced Q-Learning Training...\n",
      "Episodes: 60000, Warmup: 2000\n",
      "Experience replay: True\n",
      "Target network updates every: 500 episodes\n",
      "============================================================\n",
      "Episode 5000/60000\n",
      "  Avg Cost (last 1000): 310386.04\n",
      "  Avg TD Error: 1184.6006\n",
      "  Epsilon: 0.1115\n",
      "  Alpha: 0.1820\n",
      "  States explored: 881\n",
      "  Experience buffer: 50000\n",
      "  Exploration breakdown:\n",
      "    random: 21.1%\n",
      "    greedy: 57.8%\n",
      "    ucb: 0.0%\n",
      "    count_based: 21.1%\n",
      "\n",
      "Episode 10000/60000\n",
      "  Avg Cost (last 1000): 310514.57\n",
      "  Avg TD Error: 1191.7922\n",
      "  Epsilon: 0.0100\n",
      "  Alpha: 0.1104\n",
      "  States explored: 881\n",
      "  Experience buffer: 50000\n",
      "  Exploration breakdown:\n",
      "    random: 11.7%\n",
      "    greedy: 76.5%\n",
      "    ucb: 0.0%\n",
      "    count_based: 11.7%\n",
      "\n",
      "Episode 15000/60000\n",
      "  Avg Cost (last 1000): 309524.78\n",
      "  Avg TD Error: 1197.7817\n",
      "  Epsilon: 0.0100\n",
      "  Alpha: 0.0669\n",
      "  States explored: 881\n",
      "  Experience buffer: 50000\n",
      "  Exploration breakdown:\n",
      "    random: 8.2%\n",
      "    greedy: 83.5%\n",
      "    ucb: 0.0%\n",
      "    count_based: 8.2%\n",
      "\n",
      "Episode 20000/60000\n",
      "  Avg Cost (last 1000): 312939.10\n",
      "  Avg TD Error: 1205.8438\n",
      "  Epsilon: 0.0100\n",
      "  Alpha: 0.0406\n",
      "  States explored: 881\n",
      "  Experience buffer: 50000\n",
      "  Exploration breakdown:\n",
      "    random: 6.4%\n",
      "    greedy: 87.1%\n",
      "    ucb: 0.0%\n",
      "    count_based: 6.5%\n",
      "\n",
      "Episode 25000/60000\n",
      "  Avg Cost (last 1000): 319570.26\n",
      "  Avg TD Error: 1216.6229\n",
      "  Epsilon: 0.0100\n",
      "  Alpha: 0.0246\n",
      "  States explored: 881\n",
      "  Experience buffer: 50000\n",
      "  Exploration breakdown:\n",
      "    random: 5.4%\n",
      "    greedy: 89.3%\n",
      "    ucb: 0.0%\n",
      "    count_based: 5.4%\n",
      "\n",
      "Episode 30000/60000\n",
      "  Avg Cost (last 1000): 318735.54\n",
      "  Avg TD Error: 1216.5034\n",
      "  Epsilon: 0.0100\n",
      "  Alpha: 0.0149\n",
      "  States explored: 881\n",
      "  Experience buffer: 50000\n",
      "  Exploration breakdown:\n",
      "    random: 4.6%\n",
      "    greedy: 90.8%\n",
      "    ucb: 0.0%\n",
      "    count_based: 4.6%\n",
      "\n",
      "Episode 35000/60000\n",
      "  Avg Cost (last 1000): 320896.11\n",
      "  Avg TD Error: 1230.2225\n",
      "  Epsilon: 0.0100\n",
      "  Alpha: 0.0091\n",
      "  States explored: 881\n",
      "  Experience buffer: 50000\n",
      "  Exploration breakdown:\n",
      "    random: 4.1%\n",
      "    greedy: 91.8%\n",
      "    ucb: 0.0%\n",
      "    count_based: 4.1%\n",
      "\n",
      "Episode 40000/60000\n",
      "  Avg Cost (last 1000): 318560.27\n",
      "  Avg TD Error: 1228.3634\n",
      "  Epsilon: 0.0100\n",
      "  Alpha: 0.0055\n",
      "  States explored: 881\n",
      "  Experience buffer: 50000\n",
      "  Exploration breakdown:\n",
      "    random: 3.7%\n",
      "    greedy: 92.6%\n",
      "    ucb: 0.0%\n",
      "    count_based: 3.7%\n",
      "\n",
      "Episode 45000/60000\n",
      "  Avg Cost (last 1000): 321177.79\n",
      "  Avg TD Error: 1229.1583\n",
      "  Epsilon: 0.0100\n",
      "  Alpha: 0.0050\n",
      "  States explored: 881\n",
      "  Experience buffer: 50000\n",
      "  Exploration breakdown:\n",
      "    random: 3.4%\n",
      "    greedy: 93.3%\n",
      "    ucb: 0.0%\n",
      "    count_based: 3.4%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Run advanced Q-learning with more episodes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m advanced_agent, advanced_Q, advanced_costs = \u001b[43mrun_advanced_qlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Comprehensive evaluation\u001b[39;00m\n\u001b[32m     63\u001b[39m advanced_results = advanced_evaluation(env, advanced_Q, num_episodes=\u001b[32m1000\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_advanced_qlearning\u001b[39m\u001b[34m(env, num_episodes)\u001b[39m\n\u001b[32m      4\u001b[39m agent = AdvancedQLearning(\n\u001b[32m      5\u001b[39m     env=env,\n\u001b[32m      6\u001b[39m     initial_alpha=\u001b[32m0.3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     exploration_strategy=\u001b[33m'\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Extended training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m Q_table, episode_costs = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps_per_episode\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Longer episodes\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Longer warmup\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Analyze the learned policy\u001b[39;00m\n\u001b[32m     29\u001b[39m policy_reg, policy_spec, q_diff, analysis = agent.get_policy_analysis()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 267\u001b[39m, in \u001b[36mAdvancedQLearning.train\u001b[39m\u001b[34m(self, num_episodes, max_steps_per_episode, print_every, warmup_episodes)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28mself\u001b[39m.store_experience(state, action, reward, next_state, \n\u001b[32m    264\u001b[39m                     terminated \u001b[38;5;129;01mor\u001b[39;00m truncated)\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# Update Q-value\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m td_error = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_q_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m episode_td_errors.append(td_error)\n\u001b[32m    271\u001b[39m state = next_state\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 180\u001b[39m, in \u001b[36mAdvancedQLearning.update_q_value\u001b[39m\u001b[34m(self, state, action, reward, next_state, episode, done)\u001b[39m\n\u001b[32m    177\u001b[39m     target_q = modified_reward\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Use target network for stability\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     next_q_max = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mQ_target\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_state_tuple\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     target_q = modified_reward + \u001b[38;5;28mself\u001b[39m.gamma * next_q_max\n\u001b[32m    183\u001b[39m td_error = target_q - current_q\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Enhanced Q-Learning for Call Centre Environment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Use the same environment setup as your original code\n",
    "    max_queue_size = MAX_QUEUE_SIZE\n",
    "    drop_penalty = DROP_PENALTY\n",
    "    scaler = 10.0\n",
    "\n",
    "    arrival_means = {\n",
    "        0: ARRIVAL_REGULAR,\n",
    "        1: ARRIVAL_SPECIFIC\n",
    "    }\n",
    "    \n",
    "    service_means = {\n",
    "        (1, 0): SERVICE_REGULAR_1,\n",
    "        (1, 1): SERVICE_SPECIFIC_1,\n",
    "        (2, 0): SERVICE_REGULAR_2,\n",
    "        (2, 1): SERVICE_SPECIFIC_2\n",
    "    }\n",
    "\n",
    "    print(f\"Regular call probability: {p_regular:.3f}\")\n",
    "    print(f\"Service times (mean): Q1 Regular = {service_means[(1,0)]:.1f} s, Specific = {service_means[(1,1)]:.1f} s\")\n",
    "    print(f\"                     Q2 Regular = {service_means[(2,0)]:.1f} s, Specific = {service_means[(2,1)]:.1f} s\")\n",
    "    print(f\"Arrival times (mean): Regular = {arrival_means[0] / scaler:.1f} s, Specific = {arrival_means[1] / scaler:.1f} s\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(1901448)\n",
    "    np.random.seed(1901448)\n",
    "    \n",
    "    env = EventBasedCallCentreEnv(\n",
    "        max_queue_size=max_queue_size,\n",
    "        drop_penalty=drop_penalty,\n",
    "        arr_scaler=scaler,\n",
    "        arrival_regular_mean=arrival_means[0],\n",
    "        arrival_specific_mean=arrival_means[1],\n",
    "        service_regular_1=service_means[(1, 0)],\n",
    "        service_specific_1=service_means[(1, 1)],\n",
    "        service_regular_2=service_means[(2, 0)],\n",
    "        service_specific_2=service_means[(2, 1)],\n",
    "        seed=1901448\n",
    "    )\n",
    "\n",
    "    # Evaluate baseline first\n",
    "    print(\"\\n1. BASELINE EVALUATION:\")\n",
    "    baseline_cost, _, _ = shortest_queue_baseline(env, num_episodes=1000)\n",
    "    \n",
    "    # Train and evaluate enhanced Q-learning\n",
    "    print(\"\\n3. ENHANCED Q-LEARNING:\")\n",
    "    print(\"Training enhanced Q-learning...\")\n",
    "    \n",
    "    # Reset environment seed for fair comparison\n",
    "    env.seed(1901448)\n",
    "    \n",
    "    print(\"Advanced Q-Learning with Extended Training\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Run advanced Q-learning with more episodes\n",
    "    advanced_agent, advanced_Q, advanced_costs = run_advanced_qlearning(env, num_episodes=60000)\n",
    "\n",
    "    # Comprehensive evaluation\n",
    "    advanced_results = advanced_evaluation(env, advanced_Q, num_episodes=1000)\n",
    "\n",
    "    # Compare with baseline\n",
    "    print(\"\\nFINAL COMPARISON:\")\n",
    "    print(f\"Baseline cost:     {baseline_cost:.2f}\")\n",
    "    print(f\"Advanced Q cost:   {advanced_results['avg_cost']:.2f}\")\n",
    "    improvement = baseline_cost - advanced_results['avg_cost']\n",
    "    print(f\"Improvement:       {improvement:.2f} ({improvement/baseline_cost*100:.2f}%)\")\n",
    "\n",
    "    if advanced_results['avg_cost'] < baseline_cost:\n",
    "        print(\"✅ Enhanced Q-Learning beats baseline\")\n",
    "    else:\n",
    "        print(\"❌ Enhanced Q-Learning loses to baseline\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nTraining completed! Check the comprehensive plots above for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ea67d",
   "metadata": {},
   "source": [
    "### What to implement next:\n",
    "\n",
    "* Implement Gym Environment\n",
    "* Implement Q-Learning\n",
    "* Implement PPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
