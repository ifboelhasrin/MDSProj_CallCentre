{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d81f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ef875c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "from scipy.special import comb\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c14a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('1', 'REGULAR'): {'shape': 0.8324764719822447,\n",
       "  'loc': -3.115567764829924,\n",
       "  'scale': 123.58267067008317},\n",
       " ('1', 'SPECIFIC'): {'shape': 1.0752011949530333,\n",
       "  'loc': -1.9054138290538103,\n",
       "  'scale': 126.33203131875452},\n",
       " ('2', 'REGULAR'): {'shape': 0.8540932024632348,\n",
       "  'loc': -8.785773518703614,\n",
       "  'scale': 148.1498746014255},\n",
       " ('2', 'SPECIFIC'): {'shape': 1.0827964542219308,\n",
       "  'loc': -1.0635409275577619,\n",
       "  'scale': 96.9210654259884},\n",
       " 'REGULAR': {'shape': 1.045274579529165,\n",
       "  'loc': -3.0642813856887496,\n",
       "  'scale': 44.96983117392873},\n",
       " 'SPECIFIC': {'shape': 1.1207108865662982,\n",
       "  'loc': -5.97020631347698,\n",
       "  'scale': 111.61143284668836}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the parameters from the pickle file\n",
    "with open('pickles/anonbank_best_fit_params_report.pkl', 'rb') as f:\n",
    "    best_fit_params_df = pickle.load(f)\n",
    "\n",
    "# Convert to dictionary for easier access\n",
    "best_fit_params_dict = best_fit_params_df.set_index('param_name').T.to_dict()\n",
    "\n",
    "best_fit_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "699c40c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('1', 'REGULAR'): {'mu': 4.816910330259358,\n",
       "  'sigma': 0.006736114570356179,\n",
       "  'loc': -3.115567764829924},\n",
       " ('1', 'SPECIFIC'): {'mu': 4.838913610177206,\n",
       "  'sigma': 0.008510760998310192,\n",
       "  'loc': -1.9054138290538103},\n",
       " ('2', 'REGULAR'): {'mu': 4.9982244275894185,\n",
       "  'sigma': 0.005765014032185277,\n",
       "  'loc': -8.785773518703614},\n",
       " ('2', 'SPECIFIC'): {'mu': 4.57389688872724,\n",
       "  'sigma': 0.011171592730385803,\n",
       "  'loc': -1.0635409275577619},\n",
       " 'REGULAR': {'mu': 3.805991846582301,\n",
       "  'sigma': 0.0232407685000318,\n",
       "  'loc': -3.0642813856887496},\n",
       " 'SPECIFIC': {'mu': 4.715023489562893,\n",
       "  'sigma': 0.010040930489862581,\n",
       "  'loc': -5.97020631347698}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pickles/anonbank_lognorm_params_report.pkl', 'rb') as f:\n",
    "    lognorm_params_df = pickle.load(f)\n",
    "    \n",
    "lognorm_params_dict = lognorm_params_df.set_index('param_name').T.to_dict()\n",
    "\n",
    "lognorm_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ba74925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Queue size\n",
    "MAX_QUEUE_SIZE = 20\n",
    "\n",
    "# Discount factor for future rewards\n",
    "DISCOUNT = 0.95\n",
    "\n",
    "# Penalty for dropping calls (when queue is full)\n",
    "DROP_PENALTY = 3600.0 # 1 hour in seconds\n",
    "\n",
    "# State space: (queue 1 size, queue 2 size, call type)\n",
    "STATE_SPACE = (MAX_QUEUE_SIZE + 1, MAX_QUEUE_SIZE + 1, 2)\n",
    "\n",
    "# Call type: 0 = regular, 1 = specific\n",
    "CALL_TYPE = [0, 1]\n",
    "\n",
    "# Actions: 0 = route to queue 1, 1 = route to queue 2\n",
    "ACTIONS = [1, 2]\n",
    "\n",
    "# Arrival parameters for regular and specific calls\n",
    "ARRIVAL_REGULAR_MU = lognorm_params_dict['REGULAR']['mu']\n",
    "ARRIVAL_REGULAR_SIGMA = lognorm_params_dict['REGULAR']['sigma']\n",
    "ARRIVAL_REGULAR_LOC = lognorm_params_dict['REGULAR']['loc']\n",
    "\n",
    "ARRIVAL_SPECIFIC_MU = lognorm_params_dict['SPECIFIC']['mu']\n",
    "ARRIVAL_SPECIFIC_SIGMA = lognorm_params_dict['SPECIFIC']['sigma']\n",
    "ARRIVAL_SPECIFIC_LOC = lognorm_params_dict['SPECIFIC']['loc']\n",
    "\n",
    "SERVICE_REGULAR_1_MU = lognorm_params_dict[('1', 'REGULAR')]['mu']\n",
    "SERVICE_REGULAR_1_SIGMA = lognorm_params_dict[('1', 'REGULAR')]['sigma']\n",
    "SERVICE_REGULAR_1_LOC = lognorm_params_dict[('1', 'REGULAR')]['loc']\n",
    "SERVICE_SPECIFIC_1_MU = lognorm_params_dict[('1', 'SPECIFIC')]['mu']\n",
    "SERVICE_SPECIFIC_1_SIGMA = lognorm_params_dict[('1', 'SPECIFIC')]['sigma']\n",
    "SERVICE_SPECIFIC_1_LOC = lognorm_params_dict[('1', 'SPECIFIC')]['loc']\n",
    "\n",
    "SERVICE_REGULAR_2_MU = lognorm_params_dict[('2', 'REGULAR')]['mu']\n",
    "SERVICE_REGULAR_2_SIGMA = lognorm_params_dict[('2', 'REGULAR')]['sigma']\n",
    "SERVICE_REGULAR_2_LOC = lognorm_params_dict[('2', 'REGULAR')]['loc']\n",
    "SERVICE_SPECIFIC_2_MU = lognorm_params_dict[('2', 'SPECIFIC')]['mu']\n",
    "SERVICE_SPECIFIC_2_SIGMA = lognorm_params_dict[('2', 'SPECIFIC')]['sigma']\n",
    "SERVICE_SPECIFIC_2_LOC = lognorm_params_dict[('2', 'SPECIFIC')]['loc']\n",
    "\n",
    "def mean_shifted_lognormal(mu, sigma, loc):\n",
    "    \"\"\"\n",
    "    Calculate mean of shifted lognormal distribution:\n",
    "    E[X] = exp(mu + sigma^2 / 2) + loc\n",
    "    \"\"\"\n",
    "    return np.exp(mu + (sigma**2) / 2) + loc\n",
    "\n",
    "# Arrival rates\n",
    "\n",
    "ARRIVAL_REGULAR = mean_shifted_lognormal(\n",
    "    ARRIVAL_REGULAR_MU,\n",
    "    ARRIVAL_REGULAR_SIGMA,\n",
    "    ARRIVAL_REGULAR_LOC\n",
    ")\n",
    "\n",
    "ARRIVAL_SPECIFIC = mean_shifted_lognormal(\n",
    "    ARRIVAL_SPECIFIC_MU,\n",
    "    ARRIVAL_SPECIFIC_SIGMA,\n",
    "    ARRIVAL_SPECIFIC_LOC\n",
    ")\n",
    "\n",
    "# Service rates\n",
    "\n",
    "SERVICE_REGULAR_1 = mean_shifted_lognormal(\n",
    "    SERVICE_REGULAR_1_MU,\n",
    "    SERVICE_REGULAR_1_SIGMA,\n",
    "    SERVICE_REGULAR_1_LOC\n",
    ")\n",
    "\n",
    "SERVICE_REGULAR_2 = mean_shifted_lognormal(\n",
    "    SERVICE_REGULAR_2_MU,\n",
    "    SERVICE_REGULAR_2_SIGMA,\n",
    "    SERVICE_REGULAR_2_LOC\n",
    ")\n",
    "\n",
    "SERVICE_SPECIFIC_1 = mean_shifted_lognormal(\n",
    "    SERVICE_SPECIFIC_1_MU,\n",
    "    SERVICE_SPECIFIC_1_SIGMA,\n",
    "    SERVICE_SPECIFIC_1_LOC\n",
    ")\n",
    "\n",
    "SERVICE_SPECIFIC_2 = mean_shifted_lognormal(\n",
    "    SERVICE_SPECIFIC_2_MU,\n",
    "    SERVICE_SPECIFIC_2_SIGMA,\n",
    "    SERVICE_SPECIFIC_2_LOC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79303f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARRIVAL_REGULAR: 41.917696280463964, ARRIVAL_SPECIFIC: 105.64685302326639\n",
      "SERVICE_REGULAR_1: 120.46990673369942, SERVICE_REGULAR_2: 139.36656301237034\n",
      "SERVICE_SPECIFIC_1: 124.43119287989809, SERVICE_SPECIFIC_2: 95.86357277892792\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARRIVAL_REGULAR: {ARRIVAL_REGULAR}, ARRIVAL_SPECIFIC: {ARRIVAL_SPECIFIC}\")\n",
    "print(f\"SERVICE_REGULAR_1: {SERVICE_REGULAR_1}, SERVICE_REGULAR_2: {SERVICE_REGULAR_2}\")\n",
    "print(f\"SERVICE_SPECIFIC_1: {SERVICE_SPECIFIC_1}, SERVICE_SPECIFIC_2: {SERVICE_SPECIFIC_2}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efeeaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lognormal expected value calculation\n",
    "\n",
    "def expected_waiting_time_lognormal(q_size, p_regular, mean_s, mean_c):\n",
    "    \"\"\"\n",
    "    Calculate expected waiting time for a queue with q_size calls,\n",
    "    each call being regular with probability p_regular,\n",
    "    using direct mean service times for regular and specific calls.\n",
    "    \"\"\"\n",
    "    wait = 0.0\n",
    "    for k in range(q_size + 1):\n",
    "        # Binomial probability of k regular calls in the queue\n",
    "        prob = comb(q_size, k) * (p_regular ** k) * ((1 - p_regular) ** (q_size - k))\n",
    "        # Waiting time if k regular and (q_size - k) specific ahead\n",
    "        wait_k = k * mean_s + (q_size - k) * mean_c\n",
    "        wait += prob * wait_k\n",
    "\n",
    "    # Add mean service time for the current arriving call (assumed regular here)\n",
    "    wait += mean_s\n",
    "    return wait\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69235657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected waiting time at queue 1: 1156.5679\n",
      "Expected waiting time at queue 2: 1425.3833\n"
     ]
    }
   ],
   "source": [
    "# Example usage to calculate expected waiting time for queue 1:\n",
    "\n",
    "rate_regular = 1 / ARRIVAL_REGULAR\n",
    "rate_specific = 1 / ARRIVAL_SPECIFIC\n",
    "\n",
    "p_regular = rate_regular / (rate_regular + rate_specific)\n",
    "\n",
    "mean_s1 = SERVICE_REGULAR_1\n",
    "\n",
    "mean_c1 = SERVICE_SPECIFIC_1\n",
    "\n",
    "q1 = 10 # Example queue size\n",
    "\n",
    "expected_wait = expected_waiting_time_lognormal(q1, p_regular, mean_s1, mean_c1)\n",
    "print(f\"Expected waiting time at queue 1: {expected_wait:.4f}\")\n",
    "\n",
    "# Compare with queue 2\n",
    "\n",
    "mean_s2 = SERVICE_REGULAR_2\n",
    "\n",
    "mean_c2 = SERVICE_SPECIFIC_2\n",
    "\n",
    "expected_wait = expected_waiting_time_lognormal(q1, p_regular, mean_s2, mean_c2) # Assuming same queue size for simplicity\n",
    "print(f\"Expected waiting time at queue 2: {expected_wait:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef37be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallCentreMDPEnv:\n",
    "    \"\"\"\n",
    "    RL environment for a two-queue call centre.\n",
    "    State: (q1_length, q2_length, call_type)\n",
    "    Action: 1 = route to queue 1, 2 = queue 2\n",
    "    Reward: Negative expected waiting time (cost to minimize)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_queue=MAX_QUEUE_SIZE, drop_penalty=DROP_PENALTY, seed=1901448):\n",
    "        self.max_queue = max_queue\n",
    "        self.drop_penalty = drop_penalty\n",
    "        self.state = None\n",
    "\n",
    "        # Precompute arrival rates and squared coefficients of variation (cv^2)\n",
    "        self.lam_arrival = {\n",
    "            0: 1 / lognormal_mean(ARRIVAL_REGULAR_MU, ARRIVAL_REGULAR_SIGMA, ARRIVAL_REGULAR_LOC),\n",
    "            1: 1 / lognormal_mean(ARRIVAL_SPECIFIC_MU, ARRIVAL_SPECIFIC_SIGMA, ARRIVAL_SPECIFIC_LOC)\n",
    "        }\n",
    "        self.ca2_arrival = {\n",
    "            0: lognormal_cv2(ARRIVAL_REGULAR_MU, ARRIVAL_REGULAR_SIGMA, ARRIVAL_REGULAR_LOC),\n",
    "            1: lognormal_cv2(ARRIVAL_SPECIFIC_MU, ARRIVAL_SPECIFIC_SIGMA, ARRIVAL_SPECIFIC_LOC)\n",
    "        }\n",
    "\n",
    "        # Store service lognormal parameters per queue and call type\n",
    "        self.service_params = {\n",
    "            (1, 0): (SERVICE_REGULAR_1_MU, SERVICE_REGULAR_1_SIGMA, SERVICE_REGULAR_1_LOC),\n",
    "            (1, 1): (SERVICE_SPECIFIC_1_MU, SERVICE_SPECIFIC_1_SIGMA, SERVICE_SPECIFIC_1_LOC),\n",
    "            (2, 0): (SERVICE_REGULAR_2_MU, SERVICE_REGULAR_2_SIGMA, SERVICE_REGULAR_2_LOC),\n",
    "            (2, 1): (SERVICE_SPECIFIC_2_MU, SERVICE_SPECIFIC_2_SIGMA, SERVICE_SPECIFIC_2_LOC),\n",
    "        }\n",
    "        self.service_means = {}\n",
    "        self.service_cv2 = {}\n",
    "\n",
    "        for key, (mu, sigma, loc) in self.service_params.items():\n",
    "            self.service_means[key] = lognormal_mean(mu, sigma, loc)\n",
    "            self.service_cv2[key] = lognormal_cv2(mu, sigma, loc)\n",
    "\n",
    "        # Initialize queues and tracked service times\n",
    "        self.queue1_length = 0\n",
    "        self.queue2_length = 0\n",
    "        self.queue1_service_times = []\n",
    "        self.queue2_service_times = []\n",
    "\n",
    "        self.seed(seed)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "\n",
    "    def reset(self):\n",
    "        self.queue1_length = 0\n",
    "        self.queue2_length = 0\n",
    "        self.queue1_service_times = []\n",
    "        self.queue2_service_times = []\n",
    "\n",
    "        total_lam = self.lam_arrival[0] + self.lam_arrival[1]\n",
    "        call_type = np.random.choice([0, 1], p=[self.lam_arrival[0]/total_lam, self.lam_arrival[1]/total_lam])\n",
    "        self.state = (0, 0, call_type)\n",
    "        return self.state\n",
    "\n",
    "    def _kingman_expected_waiting(self, lam, mu_s, ca2, cs2):\n",
    "        if lam <= 0:\n",
    "            return 0.0\n",
    "        rho = lam * mu_s\n",
    "        if rho >= 1.0:\n",
    "            return float('inf')  # unstable system\n",
    "        return (rho / (1 - rho)) * (ca2 + cs2) / 2 * mu_s\n",
    "\n",
    "    def _expected_waiting_time(self, queue_num, queue_length, call_type):\n",
    "        if queue_length == 0:\n",
    "            return 0.0\n",
    "        lam = self.lam_arrival[call_type]\n",
    "        ca2 = self.ca2_arrival[call_type]\n",
    "        mu_s = self.service_means[(queue_num, call_type)]\n",
    "        cs2 = self.service_cv2[(queue_num, call_type)]\n",
    "        expected_wait_one_call = self._kingman_expected_waiting(lam, mu_s, ca2, cs2)\n",
    "        return queue_length * expected_wait_one_call\n",
    "\n",
    "    def _sample_service_time(self, queue_num, call_type):\n",
    "        mu, sigma, loc = self.service_params[(queue_num, call_type)]\n",
    "        return np.random.lognormal(mean=mu, sigma=sigma) + loc\n",
    "\n",
    "    def step(self, action):\n",
    "        q1_len, q2_len, call_type = self.state\n",
    "        dropped = False\n",
    "\n",
    "        if action == 1 and q1_len >= self.max_queue:\n",
    "            reward = -self.drop_penalty\n",
    "            dropped = True\n",
    "        elif action == 2 and q2_len >= self.max_queue:\n",
    "            reward = -self.drop_penalty\n",
    "            dropped = True\n",
    "        else:\n",
    "            if action == 1:\n",
    "                expected_wait = self._expected_waiting_time(1, q1_len, call_type)\n",
    "                self.queue1_length = min(q1_len + 1, self.max_queue)\n",
    "                new_service = self._sample_service_time(1, call_type)\n",
    "                self.queue1_service_times.append(new_service)\n",
    "            elif action == 2:\n",
    "                expected_wait = self._expected_waiting_time(2, q2_len, call_type)\n",
    "                self.queue2_length = min(q2_len + 1, self.max_queue)\n",
    "                new_service = self._sample_service_time(2, call_type)\n",
    "                self.queue2_service_times.append(new_service)\n",
    "\n",
    "            reward = -expected_wait\n",
    "            dropped = False\n",
    "\n",
    "        # Advance time by 1 unit for all calls, remove completed calls\n",
    "        time_step = 1.0\n",
    "        self.queue1_service_times = [t - time_step for t in self.queue1_service_times if t - time_step > 0]\n",
    "        self.queue2_service_times = [t - time_step for t in self.queue2_service_times if t - time_step > 0]\n",
    "\n",
    "        self.queue1_length = len(self.queue1_service_times)\n",
    "        self.queue2_length = len(self.queue2_service_times)\n",
    "\n",
    "        total_lam = self.lam_arrival[0] + self.lam_arrival[1]\n",
    "        p_regular = self.lam_arrival[0] / total_lam\n",
    "        next_call_type = np.random.choice([0, 1], p=[p_regular, 1 - p_regular])\n",
    "\n",
    "        self.state = (self.queue1_length, self.queue2_length, next_call_type)\n",
    "\n",
    "        done = False\n",
    "        info = {\"dropped\": dropped}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "\n",
    "# Helper functions for lognormal calculations\n",
    "\n",
    "def lognormal_mean(mu, sigma, loc=0):\n",
    "    return np.exp(mu + sigma**2 / 2) + loc\n",
    "\n",
    "def lognormal_var(mu, sigma):\n",
    "    return (np.exp(sigma**2) - 1) * np.exp(2 * mu + sigma**2)\n",
    "\n",
    "def lognormal_cv2(mu, sigma, loc=0):\n",
    "    m = lognormal_mean(mu, sigma, loc)\n",
    "    v = lognormal_var(mu, sigma)\n",
    "    return v / m**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "439ef65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_with_debugging(\n",
    "    env,\n",
    "    num_episodes=5000,\n",
    "    max_steps_per_episode=200,\n",
    "    alpha=0.1,\n",
    "    gamma=0.9,  # Lower discount for immediate rewards\n",
    "    epsilon_start=0.9,  # Start with less exploration\n",
    "    epsilon_min=0.05,\n",
    "    epsilon_decay=0.995\n",
    "):\n",
    "    \"\"\"\n",
    "    Q-learning with debugging information\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(2))\n",
    "    \n",
    "    episode_costs = []  # Track actual costs (positive values)\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state_tuple = tuple(state)\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice([1, 2])\n",
    "            else:\n",
    "                best_action_idx = np.argmax(Q[state_tuple])\n",
    "                action = best_action_idx + 1\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state_tuple = tuple(next_state)\n",
    "            \n",
    "            # Q-learning update\n",
    "            best_next_action = np.argmax(Q[next_state_tuple])\n",
    "            td_target = reward + gamma * Q[next_state_tuple][best_next_action]\n",
    "            td_error = td_target - Q[state_tuple][action - 1]\n",
    "            Q[state_tuple][action - 1] += alpha * td_error\n",
    "            \n",
    "            state_tuple = next_state_tuple\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        episode_costs.append(-total_reward)  # Convert to positive cost\n",
    "        \n",
    "        # Progress logging\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            avg_cost = np.mean(episode_costs[-1000:])  # Average cost over last 1000 episodes\n",
    "            print(f\"Episode {episode+1}/{num_episodes}\")\n",
    "            print(f\"  Avg Cost (last 1000): {avg_cost:.2f}\")\n",
    "            print(f\"  Epsilon: {epsilon:.4f}\")\n",
    "            print(f\"  States explored: {len(Q)}\")\n",
    "            print()\n",
    "    \n",
    "    return Q, episode_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2b7e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q_table, num_episodes=100, policy_name=\"Q-Learning\"):\n",
    "    \"\"\"Evaluate a policy without exploration\"\"\"\n",
    "    total_costs = []\n",
    "    dropped_calls = 0\n",
    "    total_calls = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_cost = 0\n",
    "        \n",
    "        for step in range(200):  # Max steps per episode\n",
    "            state_tuple = tuple(state)\n",
    "            \n",
    "            # Use learned policy (no exploration)\n",
    "            if state_tuple in Q_table:\n",
    "                action_idx = np.argmax(Q_table[state_tuple])\n",
    "                action = action_idx + 1  # Convert zero-based index to action (1 or 2)\n",
    "            else:\n",
    "                action = 1  # Default to queue 1 if state not seen\n",
    "            \n",
    "            state, reward, done, info = env.step(action)\n",
    "            episode_cost += (-reward)  # Convert reward to positive cost\n",
    "            total_calls += 1\n",
    "            \n",
    "            if info[\"dropped\"]:\n",
    "                dropped_calls += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        total_costs.append(episode_cost)\n",
    "    \n",
    "    avg_cost = np.mean(total_costs)\n",
    "    std_cost = np.std(total_costs)\n",
    "    drop_rate = dropped_calls / total_calls if total_calls > 0 else 0\n",
    "    \n",
    "    print(f\"{policy_name} Policy Evaluation:\")\n",
    "    print(f\"  Average cost per episode: {avg_cost:.2f} ± {std_cost:.2f}\")\n",
    "    print(f\"  Drop rate: {drop_rate:.4f} ({dropped_calls}/{total_calls})\")\n",
    "    \n",
    "    return avg_cost, std_cost, drop_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b779c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_queue_baseline(env, num_episodes=100):\n",
    "    \"\"\"Baseline: always choose the shorter queue\"\"\"\n",
    "    total_costs = []\n",
    "    dropped_calls = 0\n",
    "    total_calls = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_cost = 0\n",
    "        \n",
    "        for step in range(200):\n",
    "            q1, q2, call_type = state\n",
    "            if q1 < q2:\n",
    "                action = 1\n",
    "            elif q2 < q1:\n",
    "                action = 2\n",
    "            else:\n",
    "                # Tie-break based on call type\n",
    "                action = 1 if call_type == 0 else 2\n",
    "            \n",
    "            state, reward, done, info = env.step(action)\n",
    "            episode_cost += (-reward)  # Convert to positive cost\n",
    "            total_calls += 1\n",
    "            \n",
    "            if info[\"dropped\"]:\n",
    "                dropped_calls += 1\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        total_costs.append(episode_cost)\n",
    "    \n",
    "    avg_cost = np.mean(total_costs)\n",
    "    std_cost = np.std(total_costs)\n",
    "    drop_rate = dropped_calls / total_calls if total_calls > 0 else 0\n",
    "    \n",
    "    print(\"Shortest Queue Baseline:\")\n",
    "    print(f\"  Average cost per episode: {avg_cost:.2f} ± {std_cost:.2f}\")\n",
    "    print(f\"  Drop rate: {drop_rate:.4f} ({dropped_calls}/{total_calls})\")\n",
    "    \n",
    "    return avg_cost, std_cost, drop_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff732357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call Centre MDP Environment\n",
      "Regular call probability: 0.713\n",
      "Service times (mean): Q1 Regular = 106.7 s, Specific = 100.9 s\n",
      "                      Q2 Regular = 141.3 s, Specific = 96.4 s\n",
      "Arrival times (mean inter-arrival time): Regular = 40.1 s, Specific = 99.3 s\n",
      "============================================================\n",
      "\n",
      "Testing environment:\n",
      "Initial state: (0, 0, np.int64(1))\n",
      "Step 1: Action=2, State=(0, 1, np.int64(0)), Cost=0.0, Dropped=False\n",
      "Step 2: Action=2, State=(0, 2, np.int64(0)), Cost=inf, Dropped=False\n",
      "Step 3: Action=2, State=(0, 3, np.int64(0)), Cost=inf, Dropped=False\n",
      "\n",
      "============================================================\n",
      "BASELINE EVALUATION:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irfan\\Box\\1 - Research Project\\Code\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:190: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "C:\\Users\\irfan\\AppData\\Local\\Temp\\ipykernel_30320\\757028209.py:40: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  td_error = td_target - Q[state_tuple][action - 1]\n",
      "C:\\Users\\irfan\\AppData\\Local\\Temp\\ipykernel_30320\\757028209.py:41: RuntimeWarning: invalid value encountered in scalar add\n",
      "  Q[state_tuple][action - 1] += alpha * td_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest Queue Baseline:\n",
      "  Average cost per episode: inf ± nan\n",
      "  Drop rate: 0.5978 (119555/200000)\n",
      "\n",
      "============================================================\n",
      "Q-LEARNING TRAINING:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Train Q-learning agent\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQ-LEARNING TRAINING:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m Q_table, costs = \u001b[43mq_learning_with_debugging\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9995\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Evaluate learned policy\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mq_learning_with_debugging\u001b[39m\u001b[34m(env, num_episodes, max_steps_per_episode, alpha, gamma, epsilon_start, epsilon_min, epsilon_decay)\u001b[39m\n\u001b[32m     35\u001b[39m next_state_tuple = \u001b[38;5;28mtuple\u001b[39m(next_state)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Q-learning update\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m best_next_action = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_state_tuple\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m td_target = reward + gamma * Q[next_state_tuple][best_next_action]\n\u001b[32m     40\u001b[39m td_error = td_target - Q[state_tuple][action - \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\irfan\\Box\\1 - Research Project\\Code\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:1341\u001b[39m, in \u001b[36margmax\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m   1252\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1253\u001b[39m \u001b[33;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[32m   1254\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1338\u001b[39m \u001b[33;03m(2, 1, 4)\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1340\u001b[39m kwds = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\irfan\\Box\\1 - Research Project\\Code\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Call Centre MDP Environment\")\n",
    "\n",
    "    # Compute mean service times for printing\n",
    "    reg_q1_mean = lognormal_mean(SERVICE_REGULAR_1_MU, SERVICE_REGULAR_1_SIGMA, SERVICE_REGULAR_1_LOC)\n",
    "    reg_q2_mean = lognormal_mean(SERVICE_REGULAR_2_MU, SERVICE_REGULAR_2_SIGMA, SERVICE_REGULAR_2_LOC)\n",
    "    spec_q1_mean = lognormal_mean(SERVICE_SPECIFIC_1_MU, SERVICE_SPECIFIC_1_SIGMA, SERVICE_SPECIFIC_1_LOC)\n",
    "    spec_q2_mean = lognormal_mean(SERVICE_SPECIFIC_2_MU, SERVICE_SPECIFIC_2_SIGMA, SERVICE_SPECIFIC_2_LOC)\n",
    "    arr_reg_mean = lognormal_mean(ARRIVAL_REGULAR_MU, ARRIVAL_REGULAR_SIGMA, ARRIVAL_REGULAR_LOC)\n",
    "    arr_spec_mean = lognormal_mean(ARRIVAL_SPECIFIC_MU, ARRIVAL_SPECIFIC_SIGMA, ARRIVAL_SPECIFIC_LOC)\n",
    "\n",
    "    total_lambda = (1 / lognormal_mean(ARRIVAL_REGULAR_MU, ARRIVAL_REGULAR_SIGMA, ARRIVAL_REGULAR_LOC) +\n",
    "                    1 / lognormal_mean(ARRIVAL_SPECIFIC_MU, ARRIVAL_SPECIFIC_SIGMA, ARRIVAL_SPECIFIC_LOC))\n",
    "    p_regular = (1 / lognormal_mean(ARRIVAL_REGULAR_MU, ARRIVAL_REGULAR_SIGMA, ARRIVAL_REGULAR_LOC)) / total_lambda\n",
    "\n",
    "    print(f\"Regular call probability: {p_regular:.3f}\")\n",
    "    print(f\"Service times (mean): Q1 Regular = {reg_q1_mean:.1f} s, Specific = {spec_q1_mean:.1f} s\")\n",
    "    print(f\"                      Q2 Regular = {reg_q2_mean:.1f} s, Specific = {spec_q2_mean:.1f} s\")\n",
    "    print(f\"Arrival times (mean inter-arrival time): Regular = {arr_reg_mean:.1f} s, Specific = {arr_spec_mean:.1f} s\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(1901448)\n",
    "    np.random.seed(1901448)\n",
    "\n",
    "    # Create environment with consistent drop penalty\n",
    "    env = CallCentreMDPEnv(max_queue=MAX_QUEUE_SIZE, drop_penalty=DROP_PENALTY)\n",
    "\n",
    "    # Test environment with a few random steps\n",
    "    print(\"\\nTesting environment:\")\n",
    "    state = env.reset()\n",
    "    print(f\"Initial state: {state}\")\n",
    "\n",
    "    for i in range(3):\n",
    "        action = random.choice([1, 2])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cost = -reward\n",
    "        print(f\"Step {i+1}: Action={action}, State={state}, Cost={cost:.1f}, Dropped={info['dropped']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "    # Evaluate baseline policy\n",
    "    print(\"BASELINE EVALUATION:\")\n",
    "    baseline_cost, _, _ = shortest_queue_baseline(env, num_episodes=1000)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "    # Train Q-learning agent\n",
    "    print(\"Q-LEARNING TRAINING:\")\n",
    "    Q_table, costs = q_learning_with_debugging(env, num_episodes=10000, epsilon_decay=0.9995, alpha=0.3)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Evaluate learned policy\n",
    "    print(\"LEARNED POLICY EVALUATION:\")\n",
    "    learned_cost, _, _ = evaluate_policy(env, Q_table, num_episodes=1000)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"FINAL COMPARISON:\")\n",
    "    print(f\"Baseline (Shortest Queue): {baseline_cost:.2f}\")\n",
    "    print(f\"Q-Learning Policy:         {learned_cost:.2f}\")\n",
    "    improvement = baseline_cost - learned_cost\n",
    "    print(f\"Improvement:               {improvement:.2f}\")\n",
    "\n",
    "    if improvement > 0:\n",
    "        print(\"✅ Q-Learning is BETTER (lower cost)\")\n",
    "    else:\n",
    "        print(\"❌ Q-Learning is WORSE (higher cost)\")\n",
    "\n",
    "    percent_impr = (improvement / baseline_cost) * 100 if baseline_cost > 0 else 0.0\n",
    "    print(f\"Percentage improvement:    {percent_impr:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs, alpha=0.5, label=\"Cost\")\n",
    "window = 50\n",
    "smoothed = pd.Series(costs).rolling(window).mean()\n",
    "plt.plot(smoothed, color='red', label=f\"Smoothed (window={window})\")\n",
    "plt.title(\"Q-Learning Costs Over Episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cost (Negative Waiting Time)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Q-table\n",
    "# Initialize policy arrays\n",
    "policy_regular = np.zeros((MAX_QUEUE_SIZE + 1, MAX_QUEUE_SIZE + 1), dtype=int)\n",
    "policy_specific = np.zeros((MAX_QUEUE_SIZE + 1, MAX_QUEUE_SIZE + 1), dtype=int)\n",
    "\n",
    "# Fill arrays with best actions from Q_table\n",
    "for q1 in range(MAX_QUEUE_SIZE + 1):\n",
    "    for q2 in range(MAX_QUEUE_SIZE + 1):\n",
    "        # Simple call policy\n",
    "        s_regular = (q1, q2, 0)\n",
    "        if s_regular in Q_table:\n",
    "            policy_regular[q1, q2] = np.argmax(Q_table[s_regular])\n",
    "        # Complex call policy\n",
    "        s_specific = (q1, q2, 1)\n",
    "        if s_specific in Q_table:\n",
    "            policy_specific[q1, q2] = np.argmax(Q_table[s_specific])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "# Plot Final Simple Call Policy\n",
    "sns.heatmap(np.flipud(policy_regular), cmap=\"YlGnBu\", ax=axes[0], annot=np.flipud(policy_regular),\n",
    "            cbar=False)\n",
    "axes[0].set_title('Final Simple Call Policy')\n",
    "axes[0].set_ylabel('Queue 1 Size')\n",
    "axes[0].set_xlabel('Queue 2 Size')\n",
    "axes[0].set_yticks(list(reversed(range(0, MAX_QUEUE_SIZE + 1, 5))))\n",
    "axes[0].set_yticklabels(range(0, MAX_QUEUE_SIZE + 1, 5))\n",
    "\n",
    "# Plot Final Complex Call Policy\n",
    "sns.heatmap(np.flipud(policy_specific), cmap=\"YlOrBr\", ax=axes[1], annot=np.flipud(policy_specific),\n",
    "            cbar=False)\n",
    "axes[1].set_title('Final Complex Call Policy')\n",
    "axes[1].set_ylabel('Queue 1 Size')\n",
    "axes[1].set_xlabel('Queue 2 Size')\n",
    "axes[1].set_yticks(list(reversed(range(0, MAX_QUEUE_SIZE + 1, 5))))\n",
    "axes[1].set_yticklabels(range(0, MAX_QUEUE_SIZE + 1, 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ea67d",
   "metadata": {},
   "source": [
    "### What to implement next:\n",
    "\n",
    "* Implement Gym Environment\n",
    "* Implement Q-Learning\n",
    "* Implement PPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
