{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d81f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef875c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "from scipy.special import comb\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c14a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('1', 'REGULAR'): {'shape': 0.8730464230836799,\n",
       "  'loc': -18.81216413263904,\n",
       "  'scale': 125.46378302666686},\n",
       " ('1', 'SPECIFIC'): {'shape': 1.214960359497789,\n",
       "  'loc': -6.43006717763892,\n",
       "  'scale': 107.29065148469789},\n",
       " ('2', 'REGULAR'): {'shape': 0.8338496982819321,\n",
       "  'loc': -8.97654648869145,\n",
       "  'scale': 150.28787524753483},\n",
       " ('2', 'SPECIFIC'): {'shape': 1.025678942745753,\n",
       "  'loc': -4.806412598886724,\n",
       "  'scale': 101.20889024937449},\n",
       " ('1', 'REGULAR_NOLOW'): {'shape': 0.9216145843793387,\n",
       "  'loc': 9.441854384689153,\n",
       "  'scale': 111.53017240822325},\n",
       " ('1', 'SPECIFIC_NOLOW'): {'shape': 1.1455358142639027,\n",
       "  'loc': 10.920950897381609,\n",
       "  'scale': 116.55293254226876},\n",
       " ('2', 'REGULAR_NOLOW'): {'shape': 0.918529262833772,\n",
       "  'loc': 9.315220497826276,\n",
       "  'scale': 131.2794207987643},\n",
       " ('2', 'SPECIFIC_NOLOW'): {'shape': 1.0882709227532887,\n",
       "  'loc': 11.367026593641773,\n",
       "  'scale': 93.3275789227768},\n",
       " 'REGULAR': {'shape': 1.0504388160598739,\n",
       "  'loc': -2.846758403840244,\n",
       "  'scale': 42.90266397968547},\n",
       " 'SPECIFIC': {'shape': 1.1336708937695927,\n",
       "  'loc': -5.260938954957395,\n",
       "  'scale': 104.59652892248457}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the parameters from the pickle file\n",
    "with open('pickles/anonbank_best_fit_params.pkl', 'rb') as f:\n",
    "    best_fit_params_df = pickle.load(f)\n",
    "\n",
    "# Convert to dictionary for easier access\n",
    "best_fit_params_dict = best_fit_params_df.set_index('param_name').T.to_dict()\n",
    "\n",
    "best_fit_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699c40c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('1', 'REGULAR'): {'mu': 4.832017135463374,\n",
       "  'sigma': 0.006958469079704907,\n",
       "  'loc': -18.81216413263904},\n",
       " ('1', 'SPECIFIC'): {'mu': 4.6755415208149325,\n",
       "  'sigma': 0.011323646523065594,\n",
       "  'loc': -6.43006717763892},\n",
       " ('2', 'REGULAR'): {'mu': 5.0125526231624855,\n",
       "  'sigma': 0.005548307071802354,\n",
       "  'loc': -8.97654648869145},\n",
       " ('2', 'SPECIFIC'): {'mu': 4.617186601308905,\n",
       "  'sigma': 0.010134016947128502,\n",
       "  'loc': -4.806412598886724},\n",
       " ('1', 'REGULAR_NOLOW'): {'mu': 4.71429515885142,\n",
       "  'sigma': 0.008263224492175078,\n",
       "  'loc': 9.441854384689153},\n",
       " ('1', 'SPECIFIC_NOLOW'): {'mu': 4.758345526403055,\n",
       "  'sigma': 0.009828222477376476,\n",
       "  'loc': 10.920950897381609},\n",
       " ('2', 'REGULAR_NOLOW'): {'mu': 4.8773280348187535,\n",
       "  'sigma': 0.006996664183970399,\n",
       "  'loc': 9.315220497826276},\n",
       " ('2', 'SPECIFIC_NOLOW'): {'mu': 4.536115658201609,\n",
       "  'sigma': 0.011660368179008536,\n",
       "  'loc': 11.367026593641773},\n",
       " 'REGULAR': {'mu': 3.7589339214345716,\n",
       "  'sigma': 0.024480564633599006,\n",
       "  'loc': -2.846758403840244},\n",
       " 'SPECIFIC': {'mu': 4.650110366782741,\n",
       "  'sigma': 0.01083819523964822,\n",
       "  'loc': -5.260938954957395}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pickles/anonbank_lognorm_params.pkl', 'rb') as f:\n",
    "    lognorm_params_df = pickle.load(f)\n",
    "    \n",
    "lognorm_params_dict = lognorm_params_df.set_index('param_name').T.to_dict()\n",
    "\n",
    "lognorm_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba74925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Queue size\n",
    "MAX_QUEUE_SIZE = 20\n",
    "\n",
    "# Discount factor for future rewards\n",
    "DISCOUNT = 0.95\n",
    "\n",
    "# Penalty for dropping calls (when queue is full)\n",
    "DROP_PENALTY = 3600.0 # 1 hour in seconds\n",
    "\n",
    "# State space: (queue 1 size, queue 2 size, call type)\n",
    "STATE_SPACE = (MAX_QUEUE_SIZE + 1, MAX_QUEUE_SIZE + 1, 2)\n",
    "\n",
    "# Call type: 0 = regular, 1 = specific\n",
    "CALL_TYPE = [0, 1]\n",
    "\n",
    "# Actions: 0 = route to queue 1, 1 = route to queue 2\n",
    "ACTIONS = [0, 1]\n",
    "\n",
    "# Arrival parameters for regular and specific calls\n",
    "ARRIVAL_REGULAR_MU = lognorm_params_dict['REGULAR']['mu']\n",
    "ARRIVAL_REGULAR_SIGMA = lognorm_params_dict['REGULAR']['sigma']\n",
    "ARRIVAL_REGULAR_LOC = lognorm_params_dict['REGULAR']['loc']\n",
    "\n",
    "ARRIVAL_SPECIFIC_MU = lognorm_params_dict['SPECIFIC']['mu']\n",
    "ARRIVAL_SPECIFIC_SIGMA = lognorm_params_dict['SPECIFIC']['sigma']\n",
    "ARRIVAL_SPECIFIC_LOC = lognorm_params_dict['SPECIFIC']['loc']\n",
    "\n",
    "SERVICE_REGULAR_1_MU = lognorm_params_dict[('1', 'REGULAR')]['mu']\n",
    "SERVICE_REGULAR_1_SIGMA = lognorm_params_dict[('1', 'REGULAR')]['sigma']\n",
    "SERVICE_REGULAR_1_LOC = lognorm_params_dict[('1', 'REGULAR')]['loc']\n",
    "SERVICE_SPECIFIC_1_MU = lognorm_params_dict[('1', 'SPECIFIC')]['mu']\n",
    "SERVICE_SPECIFIC_1_SIGMA = lognorm_params_dict[('1', 'SPECIFIC')]['sigma']\n",
    "SERVICE_SPECIFIC_1_LOC = lognorm_params_dict[('1', 'SPECIFIC')]['loc']\n",
    "\n",
    "SERVICE_REGULAR_2_MU = lognorm_params_dict[('2', 'REGULAR')]['mu']\n",
    "SERVICE_REGULAR_2_SIGMA = lognorm_params_dict[('2', 'REGULAR')]['sigma']\n",
    "SERVICE_REGULAR_2_LOC = lognorm_params_dict[('2', 'REGULAR')]['loc']\n",
    "SERVICE_SPECIFIC_2_MU = lognorm_params_dict[('2', 'SPECIFIC')]['mu']\n",
    "SERVICE_SPECIFIC_2_SIGMA = lognorm_params_dict[('2', 'SPECIFIC')]['sigma']\n",
    "SERVICE_SPECIFIC_2_LOC = lognorm_params_dict[('2', 'SPECIFIC')]['loc']\n",
    "\n",
    "def mean_shifted_lognormal(mu, sigma, loc):\n",
    "    \"\"\"\n",
    "    Calculate mean of shifted lognormal distribution:\n",
    "    E[X] = exp(mu + sigma^2 / 2) + loc\n",
    "    \"\"\"\n",
    "    return np.exp(mu + (sigma**2) / 2) + loc\n",
    "\n",
    "# Arrival rates\n",
    "\n",
    "ARRIVAL_REGULAR = mean_shifted_lognormal(\n",
    "    ARRIVAL_REGULAR_MU,\n",
    "    ARRIVAL_REGULAR_SIGMA,\n",
    "    ARRIVAL_REGULAR_LOC\n",
    ")\n",
    "\n",
    "ARRIVAL_SPECIFIC = mean_shifted_lognormal(\n",
    "    ARRIVAL_SPECIFIC_MU,\n",
    "    ARRIVAL_SPECIFIC_SIGMA,\n",
    "    ARRIVAL_SPECIFIC_LOC\n",
    ")\n",
    "\n",
    "# Service rates\n",
    "\n",
    "SERVICE_REGULAR_1 = mean_shifted_lognormal(\n",
    "    SERVICE_REGULAR_1_MU,\n",
    "    SERVICE_REGULAR_1_SIGMA,\n",
    "    SERVICE_REGULAR_1_LOC\n",
    ")\n",
    "\n",
    "SERVICE_REGULAR_2 = mean_shifted_lognormal(\n",
    "    SERVICE_REGULAR_2_MU,\n",
    "    SERVICE_REGULAR_2_SIGMA,\n",
    "    SERVICE_REGULAR_2_LOC\n",
    ")\n",
    "\n",
    "SERVICE_SPECIFIC_1 = mean_shifted_lognormal(\n",
    "    SERVICE_SPECIFIC_1_MU,\n",
    "    SERVICE_SPECIFIC_1_SIGMA,\n",
    "    SERVICE_SPECIFIC_1_LOC\n",
    ")\n",
    "\n",
    "SERVICE_SPECIFIC_2 = mean_shifted_lognormal(\n",
    "    SERVICE_SPECIFIC_2_MU,\n",
    "    SERVICE_SPECIFIC_2_SIGMA,\n",
    "    SERVICE_SPECIFIC_2_LOC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79303f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARRIVAL_REGULAR: 40.068763243462236, ARRIVAL_SPECIFIC: 99.34173344076834\n",
      "SERVICE_REGULAR_1: 106.65465642729778, SERVICE_REGULAR_2: 141.3136419859323\n",
      "SERVICE_SPECIFIC_1: 100.86746319788303, SERVICE_SPECIFIC_2: 96.4076747743812\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARRIVAL_REGULAR: {ARRIVAL_REGULAR}, ARRIVAL_SPECIFIC: {ARRIVAL_SPECIFIC}\")\n",
    "print(f\"SERVICE_REGULAR_1: {SERVICE_REGULAR_1}, SERVICE_REGULAR_2: {SERVICE_REGULAR_2}\")\n",
    "print(f\"SERVICE_SPECIFIC_1: {SERVICE_SPECIFIC_1}, SERVICE_SPECIFIC_2: {SERVICE_SPECIFIC_2}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeeaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lognormal expected value calculation\n",
    "\n",
    "def expected_waiting_time_lognormal(q_size, p_regular, mean_s, mean_c):\n",
    "    \"\"\"\n",
    "    Calculate expected waiting time for a queue with q_size calls,\n",
    "    each call being regular with probability p_regular,\n",
    "    using direct mean service times for regular and specific calls.\n",
    "    \"\"\"\n",
    "    wait = 0.0\n",
    "    for k in range(q_size + 1):\n",
    "        # Binomial probability of k regular calls in the queue\n",
    "        prob = comb(q_size, k) * (p_regular ** k) * ((1 - p_regular) ** (q_size - k))\n",
    "        # Waiting time if k regular and (q_size - k) specific ahead\n",
    "        wait_k = k * mean_s + (q_size - k) * mean_c\n",
    "        wait += prob * wait_k\n",
    "\n",
    "    # Add mean service time for the current arriving call (assumed regular here)\n",
    "    wait += mean_s\n",
    "    return wait\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69235657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage to calculate expected waiting time for queue 1:\n",
    "\n",
    "rate_regular = 1 / ARRIVAL_REGULAR\n",
    "rate_specific = 1 / ARRIVAL_SPECIFIC\n",
    "\n",
    "p_regular = rate_regular / (rate_regular + rate_specific)\n",
    "\n",
    "mean_s1 = SERVICE_REGULAR_1\n",
    "\n",
    "mean_c1 = SERVICE_SPECIFIC_1\n",
    "\n",
    "q1 = 10 # Example queue size\n",
    "\n",
    "expected_wait = expected_waiting_time_lognormal(q1, p_regular, mean_s1, mean_c1)\n",
    "print(f\"Expected waiting time at queue 1: {expected_wait:.4f}\")\n",
    "\n",
    "# Compare with queue 2\n",
    "\n",
    "mean_s2 = SERVICE_REGULAR_2\n",
    "\n",
    "mean_c2 = SERVICE_SPECIFIC_2\n",
    "\n",
    "expected_wait = expected_waiting_time_lognormal(q1, p_regular, mean_s2, mean_c2) # Assuming same queue size for simplicity\n",
    "print(f\"Expected waiting time at queue 2: {expected_wait:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallCentreMDPEnv:\n",
    "    \"\"\"\n",
    "    RL environment for a two-queue call centre.\n",
    "    State: (q1_length, q2_length, call_type)\n",
    "    Action: 0 = route to queue 1, 1 = queue 2\n",
    "    Reward: Negative expected waiting time (cost to minimize)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_queue=10, drop_penalty=1000.0, seed=1901448):\n",
    "        self.max_queue = max_queue\n",
    "        self.drop_penalty = drop_penalty\n",
    "        self.state = None\n",
    "        \n",
    "        # Use your actual service time means\n",
    "        self.service_means = {\n",
    "            (1, 0): SERVICE_REGULAR_1,    # Queue 1, Simple calls\n",
    "            (1, 1): SERVICE_SPECIFIC_1,   # Queue 1, Complex calls  \n",
    "            (2, 0): SERVICE_REGULAR_2,    # Queue 2, Simple calls\n",
    "            (2, 1): SERVICE_SPECIFIC_2,   # Queue 2, Complex calls\n",
    "        }\n",
    "        \n",
    "        self.p_regular = p_regular\n",
    "        \n",
    "        # Track queue lengths only (not actual service times for simplicity)\n",
    "        self.queue1_length = 0\n",
    "        self.queue2_length = 0\n",
    "        self.seed(seed)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.queue1_length = 0\n",
    "        self.queue2_length = 0\n",
    "        call_type = np.random.choice([0, 1], p=[self.p_regular, 1 - self.p_regular])\n",
    "        self.state = (0, 0, call_type)\n",
    "        return self.state\n",
    "\n",
    "    def _expected_waiting_time(self, queue_num, queue_length, call_type):\n",
    "        \"\"\"\n",
    "        Calculate expected waiting time for a call in given queue\n",
    "        \"\"\"\n",
    "        if queue_length == 0:\n",
    "            return 0.0  # No waiting if queue is empty\n",
    "        \n",
    "        # Expected service time for calls currently in queue\n",
    "        # Assume they're distributed according to arrival probabilities\n",
    "        expected_service_in_queue = (\n",
    "            self.p_regular * self.service_means[(queue_num, 0)] + \n",
    "            (1 - self.p_regular) * self.service_means[(queue_num, 1)]\n",
    "        )\n",
    "        \n",
    "        # Expected waiting time = queue_length * expected_service_time_per_call\n",
    "        return queue_length * expected_service_in_queue\n",
    "\n",
    "    def step(self, action):\n",
    "        q1_len, q2_len, call_type = self.state\n",
    "        dropped = False\n",
    "        \n",
    "        # Check if chosen queue is full\n",
    "        if action == 0 and q1_len >= self.max_queue:\n",
    "            reward = -self.drop_penalty  # High penalty for dropping calls\n",
    "            dropped = True\n",
    "        elif action == 1 and q2_len >= self.max_queue:\n",
    "            reward = -self.drop_penalty  # High penalty for dropping calls\n",
    "            dropped = True\n",
    "        else:\n",
    "            # Calculate expected waiting time for this call\n",
    "            if action == 0:  # Route to queue 1\n",
    "                expected_wait = self._expected_waiting_time(1, q1_len, call_type)\n",
    "                self.queue1_length = min(q1_len + 1, self.max_queue)\n",
    "            else:  # Route to queue 2\n",
    "                expected_wait = self._expected_waiting_time(2, q2_len, call_type)\n",
    "                self.queue2_length = min(q2_len + 1, self.max_queue)\n",
    "            \n",
    "            # Reward is negative waiting time (cost to minimize)\n",
    "            reward = -expected_wait\n",
    "            dropped = False\n",
    "\n",
    "        # Simulate some calls completing (simplified)\n",
    "        # Randomly reduce queue lengths based on completion probability\n",
    "        completion_prob = 0.3  # Probability a call completes each step\n",
    "        \n",
    "        if self.queue1_length > 0 and np.random.random() < completion_prob:\n",
    "            self.queue1_length -= 1\n",
    "        if self.queue2_length > 0 and np.random.random() < completion_prob:\n",
    "            self.queue2_length -= 1\n",
    "            \n",
    "        # Generate next call type\n",
    "        next_call_type = np.random.choice([0, 1], p=[self.p_regular, 1 - self.p_regular])\n",
    "        self.state = (self.queue1_length, self.queue2_length, next_call_type)\n",
    "        \n",
    "        done = False  # Infinite horizon\n",
    "        info = {\"dropped\": dropped}\n",
    "        \n",
    "        return self.state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ef65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_with_debugging(\n",
    "    env,\n",
    "    num_episodes=5000,\n",
    "    max_steps_per_episode=200,\n",
    "    alpha=0.1,\n",
    "    gamma=0.9,  # Lower discount for immediate rewards\n",
    "    epsilon_start=0.9,  # Start with less exploration\n",
    "    epsilon_min=0.05,\n",
    "    epsilon_decay=0.995\n",
    "):\n",
    "    \"\"\"\n",
    "    Q-learning with debugging information\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(2))\n",
    "    \n",
    "    episode_costs = []  # Track actual costs (positive values)\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state_tuple = tuple(state)\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice([0, 1])\n",
    "            else:\n",
    "                action = np.argmax(Q[state_tuple])\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state_tuple = tuple(next_state)\n",
    "            \n",
    "            # Q-learning update\n",
    "            best_next_action = np.argmax(Q[next_state_tuple])\n",
    "            td_target = reward + gamma * Q[next_state_tuple][best_next_action]\n",
    "            td_error = td_target - Q[state_tuple][action]\n",
    "            Q[state_tuple][action] += alpha * td_error\n",
    "            \n",
    "            state_tuple = next_state_tuple\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        episode_costs.append(-total_reward)  # Convert to positive cost\n",
    "        \n",
    "        # Progress logging\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            avg_cost = np.mean(episode_costs[-1000:])  # Average cost over last 1000 episodes\n",
    "            print(f\"Episode {episode+1}/{num_episodes}\")\n",
    "            print(f\"  Avg Cost (last 1000): {avg_cost:.2f}\")\n",
    "            print(f\"  Epsilon: {epsilon:.4f}\")\n",
    "            print(f\"  States explored: {len(Q)}\")\n",
    "            print()\n",
    "    \n",
    "    return Q, episode_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q_table, num_episodes=100, policy_name=\"Q-Learning\"):\n",
    "    \"\"\"Evaluate a policy without exploration\"\"\"\n",
    "    total_costs = []\n",
    "    dropped_calls = 0\n",
    "    total_calls = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_cost = 0\n",
    "        \n",
    "        for step in range(200):  # Max steps per episode\n",
    "            state_tuple = tuple(state)\n",
    "            \n",
    "            # Use learned policy (no exploration)\n",
    "            if state_tuple in Q_table:\n",
    "                action = np.argmax(Q_table[state_tuple])\n",
    "            else:\n",
    "                action = 0  # Default to queue 1 if state not seen\n",
    "            \n",
    "            state, reward, done, info = env.step(action)\n",
    "            episode_cost += (-reward)  # Convert reward to positive cost\n",
    "            total_calls += 1\n",
    "            \n",
    "            if info[\"dropped\"]:\n",
    "                dropped_calls += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        total_costs.append(episode_cost)\n",
    "    \n",
    "    avg_cost = np.mean(total_costs)\n",
    "    std_cost = np.std(total_costs)\n",
    "    drop_rate = dropped_calls / total_calls if total_calls > 0 else 0\n",
    "    \n",
    "    print(f\"{policy_name} Policy Evaluation:\")\n",
    "    print(f\"  Average cost per episode: {avg_cost:.2f} ± {std_cost:.2f}\")\n",
    "    print(f\"  Drop rate: {drop_rate:.4f} ({dropped_calls}/{total_calls})\")\n",
    "    \n",
    "    return avg_cost, std_cost, drop_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b779c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_queue_baseline(env, num_episodes=100):\n",
    "    \"\"\"Baseline: always choose the shorter queue\"\"\"\n",
    "    total_costs = []\n",
    "    dropped_calls = 0\n",
    "    total_calls = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_cost = 0\n",
    "        \n",
    "        for step in range(200):\n",
    "            q1, q2, call_type = state\n",
    "            # Choose shorter queue (tie-break to queue 1)\n",
    "            action = 0 if q1 <= q2 else 1\n",
    "            \n",
    "            state, reward, done, info = env.step(action)\n",
    "            episode_cost += (-reward)  # Convert to positive cost\n",
    "            total_calls += 1\n",
    "            \n",
    "            if info[\"dropped\"]:\n",
    "                dropped_calls += 1\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        total_costs.append(episode_cost)\n",
    "    \n",
    "    avg_cost = np.mean(total_costs)\n",
    "    std_cost = np.std(total_costs)\n",
    "    drop_rate = dropped_calls / total_calls if total_calls > 0 else 0\n",
    "    \n",
    "    print(\"Shortest Queue Baseline:\")\n",
    "    print(f\"  Average cost per episode: {avg_cost:.2f} ± {std_cost:.2f}\")\n",
    "    print(f\"  Drop rate: {drop_rate:.4f} ({dropped_calls}/{total_calls})\")\n",
    "    \n",
    "    return avg_cost, std_cost, drop_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff732357",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Call Centre MDP Environment\")\n",
    "    print(f\"Simple call probability: {p_regular:.3f}\")\n",
    "    print(f\"Service times: Simple Q1={SERVICE_REGULAR_1:.1f}, Q2={SERVICE_REGULAR_2:.1f}\")\n",
    "    print(f\"               Complex Q1={SERVICE_SPECIFIC_1:.1f}, Q2={SERVICE_SPECIFIC_2:.1f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create environment\n",
    "    env = CallCentreMDPEnv(max_queue=MAX_QUEUE_SIZE, drop_penalty=1000.0)\n",
    "    \n",
    "    # Test environment\n",
    "    print(\"\\nTesting environment:\")\n",
    "    state = env.reset()\n",
    "    print(f\"Initial state: {state}\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        action = random.choice([0, 1])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cost = -reward\n",
    "        print(f\"Step {i+1}: Action={action}, State={state}, Cost={cost:.1f}, Dropped={info['dropped']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Evaluate baseline\n",
    "    print(\"BASELINE EVALUATION:\")\n",
    "    baseline_cost, _, _ = shortest_queue_baseline(env, num_episodes=1000)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Train Q-learning\n",
    "    print(\"Q-LEARNING TRAINING:\")\n",
    "    Q_table, costs = q_learning_with_debugging(env, num_episodes=10000, epsilon_decay=0.9995, alpha=0.3)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Evaluate learned policy\n",
    "    print(\"LEARNED POLICY EVALUATION:\")\n",
    "    learned_cost, _, _ = evaluate_policy(env, Q_table, num_episodes=1000)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"FINAL COMPARISON:\")\n",
    "    print(f\"Baseline (Shortest Queue): {baseline_cost:.2f}\")\n",
    "    print(f\"Q-Learning Policy:         {learned_cost:.2f}\")\n",
    "    improvement = baseline_cost - learned_cost\n",
    "    print(f\"Improvement:               {improvement:.2f}\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(\"✅ Q-Learning is BETTER (lower cost)\")\n",
    "    else:\n",
    "        print(\"❌ Q-Learning is WORSE (higher cost)\")\n",
    "        \n",
    "    print(f\"Percentage improvement: {(improvement/baseline_cost)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs, alpha=0.5, label=\"Cost\")\n",
    "window = 50\n",
    "smoothed = pd.Series(costs).rolling(window).mean()\n",
    "plt.plot(smoothed, color='red', label=f\"Smoothed (window={window})\")\n",
    "plt.title(\"Q-Learning Costs Over Episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cost (Negative Waiting Time)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Q-table\n",
    "# Initialize policy arrays\n",
    "policy_regular = np.zeros((MAX_QUEUE_SIZE + 1, MAX_QUEUE_SIZE + 1), dtype=int)\n",
    "policy_specific = np.zeros((MAX_QUEUE_SIZE + 1, MAX_QUEUE_SIZE + 1), dtype=int)\n",
    "\n",
    "# Fill arrays with best actions from Q_table\n",
    "for q1 in range(MAX_QUEUE_SIZE + 1):\n",
    "    for q2 in range(MAX_QUEUE_SIZE + 1):\n",
    "        # Simple call policy\n",
    "        s_regular = (q1, q2, 0)\n",
    "        if s_regular in Q_table:\n",
    "            policy_regular[q1, q2] = np.argmax(Q_table[s_regular])\n",
    "        # Complex call policy\n",
    "        s_specific = (q1, q2, 1)\n",
    "        if s_specific in Q_table:\n",
    "            policy_specific[q1, q2] = np.argmax(Q_table[s_specific])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "# Plot Final Simple Call Policy\n",
    "sns.heatmap(np.flipud(policy_regular), cmap=\"YlGnBu\", ax=axes[0], annot=np.flipud(policy_regular),\n",
    "            cbar=False)\n",
    "axes[0].set_title('Final Simple Call Policy')\n",
    "axes[0].set_ylabel('Queue 1 Size')\n",
    "axes[0].set_xlabel('Queue 2 Size')\n",
    "axes[0].set_yticks(list(reversed(range(0, MAX_QUEUE_SIZE + 1, 5))))\n",
    "axes[0].set_yticklabels(range(0, MAX_QUEUE_SIZE + 1, 5))\n",
    "\n",
    "# Plot Final Complex Call Policy\n",
    "sns.heatmap(np.flipud(policy_specific), cmap=\"YlOrBr\", ax=axes[1], annot=np.flipud(policy_specific),\n",
    "            cbar=False)\n",
    "axes[1].set_title('Final Complex Call Policy')\n",
    "axes[1].set_ylabel('Queue 1 Size')\n",
    "axes[1].set_xlabel('Queue 2 Size')\n",
    "axes[1].set_yticks(list(reversed(range(0, MAX_QUEUE_SIZE + 1, 5))))\n",
    "axes[1].set_yticklabels(range(0, MAX_QUEUE_SIZE + 1, 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ea67d",
   "metadata": {},
   "source": [
    "### What to implement next:\n",
    "\n",
    "* Implement Gym Environment\n",
    "* Implement Q-Learning\n",
    "* Implement PPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
